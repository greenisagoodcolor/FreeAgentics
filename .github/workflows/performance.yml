name: Performance Testing

on:
  push:
    branches: [ master, main, develop ]
  pull_request:
    branches: [ master, main, develop ]
  schedule:
    # Run daily at 2 AM UTC for trend analysis
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      update_baseline:
        description: 'Update performance baseline'
        required: false
        default: 'false'
        type: boolean
      benchmark_filter:
        description: 'Filter benchmarks to run (regex)'
        required: false
        default: ''
        type: string

jobs:
  performance-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 45

    strategy:
      matrix:
        python-version: ["3.11", "3.12"]
      fail-fast: false

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for trend analysis

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-perf-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-perf-
          ${{ runner.os }}-pip-

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          build-essential \
          libpq-dev \
          redis-server \
          postgresql \
          postgresql-contrib

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install pytest-benchmark psutil memory-profiler pandas

    - name: Set up test database
      run: |
        sudo systemctl start postgresql
        sudo -u postgres createuser -s runner
        sudo -u postgres createdb test_db

    - name: Start Redis
      run: |
        sudo systemctl start redis-server

    - name: Export environment variables
      run: |
        echo "GIT_COMMIT=${{ github.sha }}" >> $GITHUB_ENV
        echo "GIT_BRANCH=${{ github.ref_name }}" >> $GITHUB_ENV
        echo "PYTHONPATH=${{ github.workspace }}" >> $GITHUB_ENV

    - name: Create benchmark directories
      run: |
        mkdir -p benchmarks/results
        mkdir -p benchmarks/baselines
        mkdir -p benchmarks/ci_results
        mkdir -p benchmarks/artifacts

    - name: Run performance benchmarks
      id: benchmarks
      run: |
        cd ${{ github.workspace }}

        # Build benchmark filter
        FILTER=""
        if [ -n "${{ github.event.inputs.benchmark_filter }}" ]; then
          FILTER="-k ${{ github.event.inputs.benchmark_filter }}"
        fi

        # Run benchmarks
        python -m pytest benchmarks/performance_suite.py \
          -v \
          --benchmark-only \
          --benchmark-json=benchmarks/results/benchmark_results.json \
          --benchmark-save=performance_py${{ matrix.python-version }} \
          --benchmark-save-data \
          --benchmark-warmup=on \
          --benchmark-disable-gc \
          --benchmark-min-rounds=5 \
          --benchmark-max-time=60 \
          --benchmark-group-by=func \
          --benchmark-sort=name \
          --benchmark-compare \
          --benchmark-columns=min,max,mean,stddev,median,iqr,outliers,rounds,iterations \
          $FILTER \
          || echo "benchmark_failed=true" >> $GITHUB_OUTPUT

    - name: Run additional performance tests
      if: steps.benchmarks.outputs.benchmark_failed != 'true'
      run: |
        cd ${{ github.workspace }}

        # Run integration performance tests
        python -m pytest tests/benchmarks/test_performance_regression.py \
          -v \
          --benchmark-only \
          --benchmark-json=benchmarks/results/integration_benchmarks.json

    - name: Process benchmark results
      id: process_results
      if: always()
      run: |
        cd ${{ github.workspace }}

        # Check if benchmark results exist
        if [ -f "benchmarks/results/benchmark_results.json" ]; then
          # Run CI integration to check for regressions
          python benchmarks/ci_integration.py \
            --results-file benchmarks/results/benchmark_results.json \
            --baseline-dir benchmarks/baselines \
            --output-dir benchmarks/ci_results \
            --github-comment \
            --fail-on-regression \
            || echo "regressions_detected=true" >> $GITHUB_OUTPUT

          # Parse results for summary
          if [ -f "benchmarks/ci_results/latest_regression_report.json" ]; then
            overall_status=$(jq -r '.overall_status' benchmarks/ci_results/latest_regression_report.json)
            total_benchmarks=$(jq -r '.summary.total_benchmarks' benchmarks/ci_results/latest_regression_report.json)
            regressions=$(jq -r '.summary.regressions' benchmarks/ci_results/latest_regression_report.json)
            improvements=$(jq -r '.summary.improvements' benchmarks/ci_results/latest_regression_report.json)

            echo "overall_status=$overall_status" >> $GITHUB_OUTPUT
            echo "total_benchmarks=$total_benchmarks" >> $GITHUB_OUTPUT
            echo "regressions=$regressions" >> $GITHUB_OUTPUT
            echo "improvements=$improvements" >> $GITHUB_OUTPUT
          fi
        else
          echo "No benchmark results found!"
          echo "overall_status=error" >> $GITHUB_OUTPUT
        fi

    - name: Generate performance report
      if: always()
      run: |
        cd ${{ github.workspace }}

        # Create summary report
        cat > benchmarks/ci_results/summary.md << EOF
        # Performance Test Summary

        **Python Version:** ${{ matrix.python-version }}
        **Commit:** ${{ github.sha }}
        **Branch:** ${{ github.ref_name }}
        **Status:** ${{ steps.process_results.outputs.overall_status }}

        ## Results
        - Total Benchmarks: ${{ steps.process_results.outputs.total_benchmarks }}
        - Regressions: ${{ steps.process_results.outputs.regressions }}
        - Improvements: ${{ steps.process_results.outputs.improvements }}

        EOF

        # Append detailed results if available
        if [ -f "benchmarks/ci_results/github_comment.md" ]; then
          cat benchmarks/ci_results/github_comment.md >> benchmarks/ci_results/summary.md
        fi

    - name: Upload benchmark artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-results-py${{ matrix.python-version }}-${{ github.sha }}
        path: |
          benchmarks/results/
          benchmarks/ci_results/
          benchmarks/artifacts/
        retention-days: 30

    - name: Comment on PR
      uses: actions/github-script@v6
      if: github.event_name == 'pull_request' && steps.process_results.outputs.regressions > 0
      with:
        script: |
          const fs = require('fs');

          // Read the generated comment
          const commentPath = 'benchmarks/ci_results/github_comment.md';
          if (!fs.existsSync(commentPath)) {
            console.log('No comment file found');
            return;
          }

          const comment = fs.readFileSync(commentPath, 'utf8');

          // Post comment
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

    - name: Update performance baseline
      if: |
        (github.event_name == 'push' && github.ref == 'refs/heads/main' && steps.process_results.outputs.regressions == '0') ||
        (github.event.inputs.update_baseline == 'true')
      run: |
        cd ${{ github.workspace }}

        # Update baseline
        python benchmarks/ci_integration.py \
          --results-file benchmarks/results/benchmark_results.json \
          --baseline-dir benchmarks/baselines \
          --update-baseline \
          ${{ github.event.inputs.update_baseline == 'true' && '--force-baseline' || '' }}

        # Commit updated baseline (only on main branch)
        if [ "${{ github.ref }}" == "refs/heads/main" ]; then
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add benchmarks/baselines/
          git diff --staged --quiet || git commit -m "Update performance baseline [skip ci]"
          git push
        fi

    - name: Generate GitHub Step Summary
      if: always()
      run: |
        echo "## Performance Benchmark Results (Python ${{ matrix.python-version }})" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ "${{ steps.process_results.outputs.overall_status }}" == "error" ]; then
          echo "❌ **Error:** Benchmarks failed to run" >> $GITHUB_STEP_SUMMARY
        else
          # Status emoji
          if [ "${{ steps.process_results.outputs.overall_status }}" == "fail" ]; then
            echo "❌ **Status:** Failed (Critical regressions detected)" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ steps.process_results.outputs.overall_status }}" == "warning" ]; then
            echo "⚠️ **Status:** Warning (Minor regressions detected)" >> $GITHUB_STEP_SUMMARY
          else
            echo "✅ **Status:** Passed" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Benchmarks:** ${{ steps.process_results.outputs.total_benchmarks }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Regressions:** ${{ steps.process_results.outputs.regressions }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Improvements:** ${{ steps.process_results.outputs.improvements }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Add summary file content if exists
          if [ -f "benchmarks/ci_results/summary.md" ]; then
            tail -n +7 benchmarks/ci_results/summary.md >> $GITHUB_STEP_SUMMARY
          fi
        fi

    - name: Fail on critical regressions
      if: steps.process_results.outputs.regressions_detected == 'true'
      run: |
        echo "❌ Critical performance regressions detected!"
        echo "Please review the performance report and address the regressions."
        exit 1

  performance-dashboard:
    needs: performance-tests
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.12"

    - name: Install dependencies
      run: |
        pip install pandas matplotlib seaborn plotly

    - name: Download all artifacts
      uses: actions/download-artifact@v3
      with:
        path: performance-artifacts/

    - name: Generate performance dashboard
      run: |
        cd ${{ github.workspace }}

        # Create dashboard generation script
        cat > generate_dashboard.py << 'EOF'
        import json
        import pandas as pd
        import matplotlib.pyplot as plt
        import seaborn as sns
        from pathlib import Path
        from datetime import datetime

        # Collect all performance data
        data = []
        for result_file in Path("performance-artifacts").rglob("**/benchmark_results.json"):
            with open(result_file) as f:
                result = json.load(f)
                for benchmark in result.get("benchmarks", []):
                    data.append({
                        "name": benchmark["name"],
                        "mean_ms": benchmark["stats"]["mean"] * 1000,
                        "stddev_ms": benchmark["stats"]["stddev"] * 1000,
                        "timestamp": datetime.now().isoformat()
                    })

        if data:
            df = pd.DataFrame(data)

            # Create performance chart
            plt.figure(figsize=(12, 8))
            sns.barplot(data=df, x="name", y="mean_ms")
            plt.xticks(rotation=45, ha='right')
            plt.title("Performance Benchmark Results")
            plt.ylabel("Mean Time (ms)")
            plt.tight_layout()
            plt.savefig("performance_dashboard.png")

            print(f"Generated dashboard with {len(data)} benchmarks")
        else:
            print("No performance data found")
        EOF

        python generate_dashboard.py

    - name: Upload dashboard
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-dashboard
        path: performance_dashboard.png
        retention-days: 90

  performance-profiling:
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' || contains(github.event.head_commit.message, '[profile]')

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.12"

    - name: Install profiling tools
      run: |
        pip install -r requirements.txt
        pip install py-spy memory-profiler line-profiler scalene

    - name: Run CPU profiling
      run: |
        cd ${{ github.workspace }}

        # Profile key performance benchmarks
        py-spy record -d 30 -o cpu_profile.svg -- \
          python -m pytest benchmarks/performance_suite.py::AgentSpawnBenchmarks -v

    - name: Run memory profiling
      run: |
        cd ${{ github.workspace }}

        # Memory profile
        python -m memory_profiler benchmarks/performance_suite.py

    - name: Upload profiling results
      uses: actions/upload-artifact@v3
      with:
        name: profiling-results
        path: |
          cpu_profile.svg
          *.prof
          *.dat
        retention-days: 7
