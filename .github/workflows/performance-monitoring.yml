name: üöÄ PERF-ENGINEER Performance Monitoring & Budget Enforcement

on:
  # Run on pushes to main/develop branches
  push:
    branches: [main, develop]

  # Run on all pull requests
  pull_request:
    branches: [main]

  # Allow manual triggering
  workflow_dispatch:
    inputs:
      regression_threshold:
        description: "Performance regression threshold (%)"
        required: false
        default: "10.0"
        type: string
      force_baseline_update:
        description: "Force baseline update"
        required: false
        default: false
        type: boolean

  # Run daily for continuous monitoring
  schedule:
    - cron: "0 2 * * *"

env:
  PYTHON_VERSION: "3.11"
  PERFORMANCE_BUDGET_AGENT_SPAWN_MS: 50
  PERFORMANCE_BUDGET_PYMDP_INFERENCE_MS: 100
  PERFORMANCE_BUDGET_MEMORY_PER_AGENT_MB: 10
  PERFORMANCE_BUDGET_API_RESPONSE_MS: 200
  PERFORMANCE_BUDGET_BUNDLE_SIZE_KB: 200
  PERFORMANCE_BUDGET_LIGHTHOUSE_SCORE: 90

jobs:
  perf-engineer-validation:
    name: üéØ PERF-ENGINEER Performance Validation
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for trend analysis

      - name: üêç Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: üì¶ Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install psutil numpy

      - name: üöÄ PERF-ENGINEER Performance Validation Suite
        id: perf-validation
        run: |
          echo "üéØ Running ZERO-TOLERANCE performance validation..."
          python benchmarks/performance_validation_suite.py > performance_report.txt 2>&1
          echo "VALIDATION_EXIT_CODE=$?" >> $GITHUB_OUTPUT
          
          # Save results with commit hash
          cp performance_baseline.json perf-baseline-${{ github.sha }}.json || echo "No baseline file"
          cp performance_results_*.json perf-results-${{ github.sha }}.json || echo "No results file"

      - name: üîç Performance Regression Detection
        id: regression-check
        run: |
          echo "üîç Advanced regression detection with trend analysis..."
          
          if [ -f performance_results_*.json ]; then
            RESULTS_FILE=$(ls performance_results_*.json | head -1)
            python benchmarks/performance_regression_detector.py \
              --results-file "$RESULTS_FILE" \
              --history-file performance_history.json \
              --save-alerts regression_alerts.json > regression_report.txt 2>&1
            
            echo "REGRESSION_EXIT_CODE=$?" >> $GITHUB_OUTPUT
          else
            echo "‚ö†Ô∏è No results file found for regression detection"
            echo "REGRESSION_EXIT_CODE=0" >> $GITHUB_OUTPUT
          fi

      - name: üì¶ Frontend Performance Analysis  
        id: frontend-analysis
        continue-on-error: true
        run: |
          echo "üì¶ PERF-ENGINEER Frontend Bundle & Lighthouse Analysis..."
          
          # Setup Node.js for frontend analysis
          if [ -d "web" ] && [ -f "web/package.json" ]; then
            cd web
            npm ci --silent --ignore-scripts || npm install --silent --ignore-scripts
            npm run build || echo "Build failed, continuing with analysis..."
            cd ..
            
            python benchmarks/frontend_performance_analyzer.py > frontend_analysis.txt 2>&1
            echo "FRONTEND_EXIT_CODE=$?" >> $GITHUB_OUTPUT
          else
            echo "‚ö†Ô∏è No web directory or package.json found, skipping frontend analysis"
            echo "FRONTEND_EXIT_CODE=0" >> $GITHUB_OUTPUT
          fi

      - name: üíæ Upload Performance Artifacts
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-results-${{ github.sha }}
          path: |
            tests/performance/*_results_*.json
            tests/performance/reports/
          retention-days: 30

      - name: Upload performance charts
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-charts-${{ github.sha }}
          path: tests/performance/reports/*.png
          retention-days: 7

      - name: Comment PR with performance summary
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Find the latest CI summary file
            const reportsDir = 'tests/performance/reports';
            const files = fs.readdirSync(reportsDir);
            const summaryFiles = files.filter(f => f.startsWith('ci_summary_'));

            if (summaryFiles.length === 0) {
              console.log('No CI summary file found');
              return;
            }

            // Get the most recent summary
            const latestSummary = summaryFiles.sort().pop();
            const summaryPath = path.join(reportsDir, latestSummary);
            const summaryContent = fs.readFileSync(summaryPath, 'utf8');

            // Create or update PR comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Performance Monitoring Summary')
            );

            const commentBody = "## üéØ Performance Monitoring Results\n\n" + summaryContent + "\n\n---\n*This comment is automatically updated by the Performance Monitoring workflow.*";

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: commentBody
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: commentBody
              });
            }

      - name: Create performance baseline for main branch
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          source venv/bin/activate
          # Copy results to baseline directory
          mkdir -p tests/performance/baseline
          cp tests/performance/*_results_*.json tests/performance/baseline/ 
          echo "Performance baseline updated for commit ${{ github.sha }}"

      - name: Fail job on severe performance regression
        if: failure() && steps.performance.outcome == 'failure'
        run: |
          echo "‚ùå Performance monitoring detected severe regressions"
          echo "Check the performance reports for details"
          exit 1

  performance-comparison:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    needs: performance-analysis

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Full history for comparison

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download current results
        uses: actions/download-artifact@v3
        with:
          name: performance-results-${{ github.sha }}
          path: tests/performance/

      - name: Get baseline results from main
        run: |
          git checkout origin/main -- tests/performance/baseline/ 

      - name: Compare with baseline performance
        run: |
          python3 << 'EOF'
          import json
          import os
          from pathlib import Path

          def load_results(directory):
              results = []
              for file in Path(directory).glob('*_results_*.json'):
                  with open(file) as f:
                      data = json.load(f)
                      if isinstance(data, list):
                          results.extend(data)
                      else:
                          results.append(data)
              return results

          # Load current and baseline results
          current_results = load_results('tests/performance')
          baseline_results = load_results('tests/performance/baseline') if os.path.exists('tests/performance/baseline') else []

          print(f'Current results: {len(current_results)}')
          print(f'Baseline results: {len(baseline_results)}')

          # Simple comparison logic
          if baseline_results:
              print('‚úÖ Baseline comparison available')
          else:
              print('‚ö†Ô∏è No baseline results found for comparison')
          EOF

  notify-slack:
    runs-on: ubuntu-latest
    needs: [performance-analysis]
    if: always() && (github.ref == 'refs/heads/main' || github.event_name == 'schedule')

    steps:
      - name: Notify Slack on failure
        if: needs.performance-analysis.result == 'failure'
        uses: 8398a7/action-slack@v3
        with:
          status: failure
          text: "üö® Performance monitoring detected regressions in ${{ github.repository }}"
          webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}

      - name: Notify Slack on success
        if: needs.performance-analysis.result == 'success' && github.event_name == 'schedule'
        uses: 8398a7/action-slack@v3
        with:
          status: success
          text: "‚úÖ Weekly performance monitoring completed successfully for ${{ github.repository }}"
          webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
