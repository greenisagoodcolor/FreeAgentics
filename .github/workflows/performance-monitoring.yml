name: Performance Monitoring

on:
  schedule:
    # Run every 6 hours
    - cron: '0 */6 * * *'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to test'
        required: true
        default: 'production'
        type: choice
        options:
          - production
          - staging

permissions:
  contents: read
  issues: write

jobs:
  performance-test:
    name: Performance Testing
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install locust pytest-benchmark requests numpy pandas matplotlib seaborn

      - name: Set environment URL
        id: env
        run: |
          if [ "${{ github.event.inputs.environment || 'production' }}" = "production" ]; then
            echo "url=https://freeagentics.com" >> $GITHUB_OUTPUT
          else
            echo "url=https://staging.freeagentics.com" >> $GITHUB_OUTPUT
          fi

      - name: Run API performance tests
        run: |
          python3 << 'EOF'
          import requests
          import time
          import json
          import statistics
          from concurrent.futures import ThreadPoolExecutor, as_completed

          BASE_URL = "${{ steps.env.outputs.url }}"
          ENDPOINTS = [
              "/api/v1/health",
              "/api/v1/agents",
              "/api/v1/coalitions",
              "/api/v1/knowledge/graph"
          ]

          def test_endpoint(endpoint, num_requests=100):
              url = f"{BASE_URL}{endpoint}"
              response_times = []
              errors = 0

              for _ in range(num_requests):
                  start = time.time()
                  try:
                      response = requests.get(url, timeout=10)
                      response_time = (time.time() - start) * 1000  # ms
                      response_times.append(response_time)

                      if response.status_code >= 400:
                          errors += 1
                  except Exception as e:
                      errors += 1
                      print(f"Error testing {endpoint}: {e}")

              return {
                  "endpoint": endpoint,
                  "response_times": response_times,
                  "errors": errors,
                  "stats": {
                      "min": min(response_times) if response_times else 0,
                      "max": max(response_times) if response_times else 0,
                      "mean": statistics.mean(response_times) if response_times else 0,
                      "median": statistics.median(response_times) if response_times else 0,
                      "p95": sorted(response_times)[int(len(response_times) * 0.95)] if response_times else 0,
                      "p99": sorted(response_times)[int(len(response_times) * 0.99)] if response_times else 0
                  }
              }

          # Run tests
          results = []
          with ThreadPoolExecutor(max_workers=4) as executor:
              futures = {executor.submit(test_endpoint, endpoint): endpoint for endpoint in ENDPOINTS}
              for future in as_completed(futures):
                  results.append(future.result())

          # Save results
          with open("performance-results.json", "w") as f:
              json.dump({
                  "timestamp": time.time(),
                  "environment": "${{ github.event.inputs.environment || 'production' }}",
                  "results": results
              }, f, indent=2)

          # Check thresholds
          failed = False
          for result in results:
              print(f"\n{result['endpoint']}:")
              print(f"  Mean: {result['stats']['mean']:.2f}ms")
              print(f"  P95: {result['stats']['p95']:.2f}ms")
              print(f"  P99: {result['stats']['p99']:.2f}ms")
              print(f"  Errors: {result['errors']}")

              # Fail if P95 > 500ms or error rate > 1%
              if result['stats']['p95'] > 500 or result['errors'] > 1:
                  failed = True

          if failed:
              print("\nPerformance thresholds exceeded!")
              exit(1)
          EOF

      - name: Run load test
        run: |
          cat > locustfile.py << 'EOF'
          from locust import HttpUser, task, between

          class FreeAgenticsUser(HttpUser):
              wait_time = between(1, 3)

              @task(3)
              def health_check(self):
                  self.client.get("/api/v1/health")

              @task(2)
              def list_agents(self):
                  self.client.get("/api/v1/agents")

              @task(1)
              def get_coalitions(self):
                  self.client.get("/api/v1/coalitions")
          EOF

          # Run headless load test
          locust \
            --headless \
            --users 100 \
            --spawn-rate 10 \
            --run-time 5m \
            --host "${{ steps.env.outputs.url }}" \
            --html performance-report.html \
            --csv performance-data

      - name: Analyze results
        run: |
          python3 << 'EOF'
          import pandas as pd
          import json
          import matplotlib.pyplot as plt
          import seaborn as sns

          # Load performance results
          with open("performance-results.json", "r") as f:
              data = json.load(f)

          # Load Locust results
          stats_df = pd.read_csv("performance-data_stats.csv")

          # Create performance report
          report = {
              "summary": {
                  "environment": data["environment"],
                  "timestamp": data["timestamp"],
                  "total_requests": int(stats_df["Request Count"].sum()),
                  "failure_rate": float(stats_df["Failure Count"].sum() / stats_df["Request Count"].sum() * 100),
                  "avg_response_time": float(stats_df["Average Response Time"].mean()),
                  "max_response_time": float(stats_df["Max Response Time"].max())
              },
              "endpoints": []
          }

          for _, row in stats_df.iterrows():
              if row["Name"] != "Aggregated":
                  report["endpoints"].append({
                      "name": row["Name"],
                      "requests": int(row["Request Count"]),
                      "failures": int(row["Failure Count"]),
                      "avg_response_time": float(row["Average Response Time"]),
                      "p95_response_time": float(row["95%"]),
                      "p99_response_time": float(row["99%"])
                  })

          # Save report
          with open("performance-analysis.json", "w") as f:
              json.dump(report, f, indent=2)

          # Create visualization
          plt.figure(figsize=(10, 6))
          endpoints = [e["name"] for e in report["endpoints"]]
          response_times = [e["p95_response_time"] for e in report["endpoints"]]

          plt.bar(endpoints, response_times)
          plt.axhline(y=500, color='r', linestyle='--', label='SLA Threshold (500ms)')
          plt.xticks(rotation=45, ha='right')
          plt.ylabel('P95 Response Time (ms)')
          plt.title(f'Performance Test Results - {report["summary"]["environment"]}')
          plt.legend()
          plt.tight_layout()
          plt.savefig('performance-chart.png')

          # Check if we need to create an alert
          if report["summary"]["failure_rate"] > 1 or report["summary"]["avg_response_time"] > 500:
              print("ALERT: Performance degradation detected!")
              with open("create-issue.txt", "w") as f:
                  f.write("true")
          else:
              with open("create-issue.txt", "w") as f:
                  f.write("false")
          EOF

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: performance-results-${{ github.run_id }}
          path: |
            performance-results.json
            performance-report.html
            performance-data*.csv
            performance-analysis.json
            performance-chart.png

      - name: Create issue if performance degraded
        if: ${{ hashFiles('create-issue.txt') != '' }}
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const shouldCreateIssue = fs.readFileSync('create-issue.txt', 'utf8').trim() === 'true';

            if (shouldCreateIssue) {
              const analysis = JSON.parse(fs.readFileSync('performance-analysis.json', 'utf8'));

              let issueBody = `## Performance Degradation Detected\n\n`;
              issueBody += `**Environment:** ${analysis.summary.environment}\n`;
              issueBody += `**Date:** ${new Date(analysis.summary.timestamp * 1000).toISOString()}\n\n`;
              issueBody += `### Summary\n`;
              issueBody += `- Total Requests: ${analysis.summary.total_requests}\n`;
              issueBody += `- Failure Rate: ${analysis.summary.failure_rate.toFixed(2)}%\n`;
              issueBody += `- Average Response Time: ${analysis.summary.avg_response_time.toFixed(2)}ms\n`;
              issueBody += `- Max Response Time: ${analysis.summary.max_response_time.toFixed(2)}ms\n\n`;
              issueBody += `### Endpoint Performance\n\n`;

              analysis.endpoints.forEach(endpoint => {
                issueBody += `#### ${endpoint.name}\n`;
                issueBody += `- Requests: ${endpoint.requests}\n`;
                issueBody += `- Failures: ${endpoint.failures}\n`;
                issueBody += `- Avg Response: ${endpoint.avg_response_time.toFixed(2)}ms\n`;
                issueBody += `- P95 Response: ${endpoint.p95_response_time.toFixed(2)}ms\n`;
                issueBody += `- P99 Response: ${endpoint.p99_response_time.toFixed(2)}ms\n\n`;
              });

              issueBody += `### Action Required\n`;
              issueBody += `- Investigate the cause of performance degradation\n`;
              issueBody += `- Check recent deployments and changes\n`;
              issueBody += `- Review server metrics and logs\n`;
              issueBody += `- Consider scaling resources if needed\n\n`;
              issueBody += `[View Full Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})`;

              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: `Performance Alert: ${analysis.summary.environment} - ${new Date().toISOString().split('T')[0]}`,
                body: issueBody,
                labels: ['performance', 'alert', 'priority:high']
              });
            }

  historical-analysis:
    name: Historical Performance Analysis
    runs-on: ubuntu-latest
    needs: performance-test

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download current results
        uses: actions/download-artifact@v4
        with:
          name: performance-results-${{ github.run_id }}

      - name: Download previous results
        uses: dawidd6/action-download-artifact@v3
        with:
          workflow: performance-monitoring.yml
          workflow_conclusion: success
          name: performance-results-*
          path: historical-results
          if_no_artifact_found: warn

      - name: Generate trend analysis
        run: |
          python3 << 'EOF'
          import json
          import glob
          import matplotlib.pyplot as plt
          import pandas as pd
          from datetime import datetime

          # Load all historical results
          all_results = []
          for file in glob.glob("historical-results/**/performance-analysis.json", recursive=True):
              with open(file, "r") as f:
                  all_results.append(json.load(f))

          # Load current result
          with open("performance-analysis.json", "r") as f:
              all_results.append(json.load(f))

          # Sort by timestamp
          all_results.sort(key=lambda x: x["summary"]["timestamp"])

          # Create DataFrame for analysis
          df_data = []
          for result in all_results:
              row = {
                  "timestamp": datetime.fromtimestamp(result["summary"]["timestamp"]),
                  "avg_response_time": result["summary"]["avg_response_time"],
                  "failure_rate": result["summary"]["failure_rate"],
                  "total_requests": result["summary"]["total_requests"]
              }
              df_data.append(row)

          df = pd.DataFrame(df_data)

          # Create trend charts
          fig, axes = plt.subplots(2, 1, figsize=(12, 8))

          # Response time trend
          axes[0].plot(df["timestamp"], df["avg_response_time"], marker='o')
          axes[0].axhline(y=500, color='r', linestyle='--', label='SLA Threshold')
          axes[0].set_ylabel('Avg Response Time (ms)')
          axes[0].set_title('Response Time Trend')
          axes[0].legend()
          axes[0].grid(True, alpha=0.3)

          # Failure rate trend
          axes[1].plot(df["timestamp"], df["failure_rate"], marker='o', color='orange')
          axes[1].axhline(y=1, color='r', linestyle='--', label='1% Threshold')
          axes[1].set_ylabel('Failure Rate (%)')
          axes[1].set_title('Failure Rate Trend')
          axes[1].legend()
          axes[1].grid(True, alpha=0.3)

          plt.tight_layout()
          plt.savefig('performance-trends.png')

          # Calculate statistics
          recent_avg = df.tail(7)["avg_response_time"].mean()
          overall_avg = df["avg_response_time"].mean()

          print(f"Recent average response time (last 7 runs): {recent_avg:.2f}ms")
          print(f"Overall average response time: {overall_avg:.2f}ms")
          print(f"Performance trend: {'Degrading' if recent_avg > overall_avg * 1.1 else 'Stable'}")
          EOF

      - name: Upload trend analysis
        uses: actions/upload-artifact@v4
        with:
          name: performance-trends-${{ github.run_id }}
          path: performance-trends.png
