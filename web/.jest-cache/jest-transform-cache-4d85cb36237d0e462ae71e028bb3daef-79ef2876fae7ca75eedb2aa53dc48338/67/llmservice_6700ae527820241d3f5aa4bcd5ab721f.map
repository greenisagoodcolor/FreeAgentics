{"version":3,"sources":["/Users/matthewmoroney/builds/FreeAgentics/web/lib/llm-service.ts"],"sourcesContent":["\"use server\";\n\nimport { streamText, generateText } from \"ai\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { notFound } from \"next/navigation\";\nimport type { KnowledgeEntry } from \"@/lib/types\";\nimport { createLogger } from \"@/lib/debug-logger\";\nimport { debugLog } from \"@/lib/debug-logger\";\nimport { extractTagsFromMarkdown } from \"@/lib/utils\";\nimport {\n  LLMError,\n  ApiKeyError,\n  TimeoutError,\n  NetworkError,\n  withTimeout,\n} from \"@/lib/llm-errors\";\nimport { defaultSettings, type LLMSettings } from \"@/lib/llm-settings\";\nimport { createOpenAI } from \"@ai-sdk/openai\";\n\n// Types and configuration\nconst logger = createLogger(\"LLM-SERVICE\");\n\nlogger.info(\"[SERVER] llm-service.ts module loaded\");\n\n// Add this interface for streaming response chunks\nexport interface StreamChunk {\n  text: string;\n  isComplete: boolean;\n}\n\n// Log the defaultSettings object to check for server references\nlogger.info(\"[SERVER] defaultSettings defined as:\", {\n  ...defaultSettings,\n  hasServerRef: \"__server_ref\" in defaultSettings,\n  keys: Object.keys(defaultSettings),\n  type: typeof defaultSettings,\n});\n\n// Add this utility function for retries\nexport async function withRetry<T>(\n  operation: () => Promise<T>,\n  maxRetries = 3,\n  initialDelay = 1000,\n): Promise<T> {\n  let lastError: Error | null = null;\n  let delay = initialDelay;\n\n  for (let attempt = 0; attempt <= maxRetries; attempt++) {\n    try {\n      return await operation();\n    } catch (error) {\n      lastError = error instanceof Error ? error : new Error(String(error));\n      console.error(\n        `Operation failed (attempt ${attempt + 1}/${maxRetries + 1}):`,\n        lastError,\n      );\n\n      // Don't delay on the last attempt\n      if (attempt < maxRetries) {\n        console.log(`Retrying in ${delay}ms...`);\n        await new Promise((resolve) => setTimeout(resolve, delay));\n        delay *= 2; // Exponential backoff\n      }\n    }\n  }\n\n  throw lastError || new Error(\"Operation failed with unknown error\");\n}\n\n// Direct implementation for OpenRouter API\nasync function callOpenRouterAPI(\n  apiKey: string,\n  model: string,\n  messages: Array<{ role: string; content: string }>,\n  temperature: number = defaultSettings.temperature,\n  max_tokens: number = defaultSettings.maxTokens,\n  top_p: number = defaultSettings.topP,\n  frequency_penalty: number = defaultSettings.frequencyPenalty,\n  presence_penalty: number = defaultSettings.presencePenalty,\n) {\n  logger.info(\"[SERVER] Calling OpenRouter API with model:\", model);\n  logger.info(\"[SERVER] OpenRouter API key length:\", apiKey.length);\n  logger.info(\n    \"[SERVER] OpenRouter API key first 5 chars:\",\n    apiKey.substring(0, 5),\n  );\n  logger.info(\"[SERVER] OpenRouter parameters:\", {\n    temperature,\n    max_tokens,\n    top_p,\n    frequency_penalty,\n    presence_penalty,\n  });\n\n  try {\n    const requestBody = {\n      model,\n      messages,\n      temperature,\n      max_tokens,\n      top_p,\n      frequency_penalty,\n      presence_penalty,\n    };\n\n    logger.info(\"[SERVER] Request body:\", JSON.stringify(requestBody));\n\n    // Add timeout to the fetch request (60 seconds)\n    const fetchPromise = fetch(\n      \"https://openrouter.ai/api/v1/chat/completions\",\n      {\n        method: \"POST\",\n        headers: {\n          \"Content-Type\": \"application/json\",\n          Authorization: `Bearer ${apiKey}`,\n          \"HTTP-Referer\": \"https://vercel.com\",\n          \"X-Title\": \"Multi-agent UI Design Grid World\",\n        },\n        body: JSON.stringify(requestBody),\n      },\n    );\n\n    const response = await withTimeout(fetchPromise, 60000, \"openrouter\");\n\n    if (!response.ok) {\n      const errorText = await response.text();\n      console.error(\"[SERVER] OpenRouter API error response:\", errorText);\n      console.error(\n        \"[SERVER] Response status:\",\n        response.status,\n        response.statusText,\n      );\n      console.error(\"[SERVER] Request headers:\", {\n        \"Content-Type\": \"application/json\",\n        Authorization: `Bearer ${apiKey.substring(0, 5)}...`,\n        \"HTTP-Referer\": \"https://vercel.com\",\n        \"X-Title\": \"Multi-agent UI Design Grid World\",\n      });\n\n      let errorData;\n      try {\n        errorData = JSON.parse(errorText);\n      } catch (e) {\n        errorData = { error: { message: errorText } };\n      }\n\n      throw new NetworkError(\n        `OpenRouter API error: ${response.status} ${response.statusText}${\n          errorData ? ` - ${JSON.stringify(errorData)}` : \"\"\n        }`,\n      );\n    }\n\n    // Add timeout to the JSON parsing (5 seconds)\n    const data = await response.json();\n\n    return data.choices[0].message.content;\n  } catch (error) {\n    console.error(\"[SERVER] Error calling OpenRouter API:\", error);\n    throw error;\n  }\n}\n\n// Generate a response using a system prompt\nexport async function generateResponse(\n  userPrompt: string,\n  systemPrompt: string,\n  settings: Partial<LLMSettings> = {},\n): Promise<string> {\n  try {\n    // CRITICAL FIX: Add detailed logging for provider and API key\n    debugLog(\n      `[LLM SERVICE] generateResponse called with provider: ${settings.provider}`,\n    );\n    debugLog(\n      `[LLM SERVICE] API key available: ${!!settings.apiKey}, length: ${settings.apiKey?.length || 0}`,\n    );\n\n    // Ensure provider is set\n    if (!settings.provider) {\n      debugLog(\"[LLM SERVICE] No provider specified, defaulting to openai\");\n      settings.provider = \"openai\";\n    }\n\n    // Log the incoming settings to check for server references\n    logger.info(\"[SERVER] generateResponse called with settings:\", {\n      ...settings,\n      apiKey: settings.apiKey\n        ? `[Length: ${settings.apiKey.length}]`\n        : undefined,\n      hasServerRef: \"__server_ref\" in settings,\n      keys: Object.keys(settings),\n    });\n\n    // Ensure we have complete settings by merging with defaults\n    const completeSettings = { ...defaultSettings, ...settings };\n\n    logger.info(\"[SERVER] completeSettings after merge:\", {\n      ...completeSettings,\n      apiKey: completeSettings.apiKey\n        ? `[Length: ${completeSettings.apiKey.length}]`\n        : undefined,\n      hasServerRef: \"__server_ref\" in completeSettings,\n      keys: Object.keys(completeSettings),\n    });\n\n    logger.info(\"[SERVER] generateResponse called with settings:\", {\n      provider: completeSettings.provider,\n      model: completeSettings.model,\n      temperature: completeSettings.temperature,\n      maxTokens: completeSettings.maxTokens,\n      topP: completeSettings.topP,\n      frequencyPenalty: completeSettings.frequencyPenalty,\n      presencePenalty: completeSettings.presencePenalty,\n      apiKeyLength: completeSettings.apiKey\n        ? completeSettings.apiKey.length\n        : 0,\n    });\n\n    // Check if API key is available\n    if (!completeSettings.apiKey) {\n      throw new ApiKeyError(completeSettings.provider);\n    }\n\n    // For OpenRouter, use our direct implementation\n    if (completeSettings.provider === \"openrouter\") {\n      logger.info(\"[SERVER] Using OpenRouter implementation\");\n      const messages: Array<{ role: string; content: string }> = [];\n      if (systemPrompt) {\n        messages.push({ role: \"system\", content: systemPrompt });\n      }\n      messages.push({ role: \"user\", content: userPrompt });\n\n      // Add retry logic for OpenRouter calls\n      return await withRetry(\n        () =>\n          callOpenRouterAPI(\n            completeSettings.apiKey!,\n            completeSettings.model,\n            messages,\n            completeSettings.temperature,\n            completeSettings.maxTokens,\n            completeSettings.topP,\n            completeSettings.frequencyPenalty,\n            completeSettings.presencePenalty,\n          ),\n        2, // Max 2 retries\n        1000, // Initial delay of 1 second\n      );\n    } else if (completeSettings.provider === \"openai\") {\n      logger.info(\"[SERVER] Using OpenAI implementation\");\n      // For OpenAI, use the AI SDK\n      const openaiProvider = createOpenAI({\n        apiKey: completeSettings.apiKey,\n      });\n      const model = openaiProvider(completeSettings.model);\n\n      // Add timeout to the OpenAI call\n      const result = await withTimeout(\n        generateText({\n          model,\n          system: systemPrompt,\n          prompt: userPrompt,\n          temperature: completeSettings.temperature,\n          maxTokens: completeSettings.maxTokens,\n          topP: completeSettings.topP,\n          frequencyPenalty: completeSettings.frequencyPenalty,\n          presencePenalty: completeSettings.presencePenalty,\n        }),\n        60000,\n        \"OpenAI API request timed out after 60 seconds\",\n      );\n\n      return result.text;\n    } else {\n      throw new LLMError(\n        `Unsupported provider: ${completeSettings.provider}`,\n        \"unknown\",\n      );\n    }\n  } catch (error) {\n    logger.error(\"[SERVER] Error in generateResponse:\", error);\n    throw error;\n  }\n}\n\n// Add more detailed logging to streamGenerateResponse function\nexport async function* streamGenerateResponse(\n  systemPrompt: string,\n  userPrompt: string,\n  settings: LLMSettings,\n): AsyncGenerator<StreamChunk, void, unknown> {\n  try {\n    logger.info(\"[SERVER] streamGenerateResponse function called\");\n    logger.info(\"[SERVER] streamGenerateResponse parameters:\", {\n      systemPromptLength: systemPrompt?.length,\n      userPromptLength: userPrompt?.length,\n      settingsProvider: settings?.provider,\n      settingsModel: settings?.model,\n    });\n\n    // Ensure we have complete settings by merging with defaults\n    const completeSettings = { ...defaultSettings, ...settings };\n\n    logger.info(\"[SERVER] streamGenerateResponse called with settings:\", {\n      provider: completeSettings.provider,\n      model: completeSettings.model,\n      temperature: completeSettings.temperature,\n      apiKeyLength: completeSettings.apiKey\n        ? completeSettings.apiKey.length\n        : 0,\n    });\n\n    // Rest of the function...\n    // Improved streaming response generation with better async iterable implementation\n    // export async function* streamGenerateResponse(\n    //   systemPrompt: string,\n    //   userPrompt: string,\n    //   settings: LLMSettings,\n    // ): AsyncGenerator<ResponseChunk, void, unknown> {\n    //   try {\n    //     // Ensure we have complete settings by merging with defaults\n    //     const completeSettings = { ...defaultSettings, ...settings }\n\n    //     logger.info(\"[SERVER] streamGenerateResponse called with settings:\", {\n    //       provider: completeSettings.provider,\n    //       model: completeSettings.model,\n    //       temperature: completeSettings.temperature,\n    //       apiKeyLength: completeSettings.apiKey ? completeSettings.apiKey.length : 0,\n    //     })\n\n    // Check if API key is available\n    if (!completeSettings.apiKey) {\n      yield {\n        text: `Error: API key is required for ${completeSettings.provider} provider`,\n        isComplete: true,\n      };\n      return;\n    }\n\n    if (completeSettings.provider === \"openai\") {\n      logger.info(\"[SERVER] Using OpenAI streaming implementation\");\n\n      try {\n        const model = openai(completeSettings.model as any);\n\n        // Use a fallback mechanism in case streaming fails\n        let streamFailed = false;\n        let fullText = \"\";\n\n        try {\n          const stream = await streamText({\n            model,\n            system: systemPrompt,\n            prompt: userPrompt,\n          });\n\n          for await (const chunk of stream.textStream) {\n            fullText += chunk;\n            yield {\n              text: chunk,\n              isComplete: false,\n            };\n          }\n        } catch (streamError) {\n          console.error(\n            \"[SERVER] Error in OpenAI streaming, falling back to non-streaming:\",\n            streamError,\n          );\n          streamFailed = true;\n        }\n\n        // If streaming failed, fall back to non-streaming\n        if (streamFailed) {\n          const { text } = await generateText({\n            model,\n            system: systemPrompt,\n            prompt: userPrompt,\n          });\n\n          yield {\n            text,\n            isComplete: false,\n          };\n        }\n\n        yield {\n          text: \"\",\n          isComplete: true,\n        };\n      } catch (error) {\n        console.error(\"[SERVER] Error in OpenAI response generation:\", error);\n        yield {\n          text: `Error: ${error instanceof Error ? error.message : String(error)}`,\n          isComplete: true,\n        };\n      }\n    } else if (completeSettings.provider === \"openrouter\") {\n      // For OpenRouter, implement streaming using their API\n      logger.info(\"[SERVER] Using OpenRouter streaming implementation\");\n\n      try {\n        const messages = [];\n        if (systemPrompt) {\n          messages.push({ role: \"system\", content: systemPrompt });\n        }\n        messages.push({ role: \"user\", content: userPrompt });\n\n        // First try streaming\n        let streamFailed = false;\n        let fullResponse = \"\";\n\n        try {\n          const response = await fetch(\n            \"https://openrouter.ai/api/v1/chat/completions\",\n            {\n              method: \"POST\",\n              headers: {\n                \"Content-Type\": \"application/json\",\n                Authorization: `Bearer ${completeSettings.apiKey}`,\n                \"HTTP-Referer\": \"https://vercel.com\",\n                \"X-Title\": \"Multi-agent UI Design Grid World\",\n              },\n              body: JSON.stringify({\n                model: completeSettings.model,\n                messages,\n                temperature: completeSettings.temperature,\n                max_tokens: completeSettings.maxTokens,\n                top_p: completeSettings.topP,\n                frequency_penalty: completeSettings.frequencyPenalty,\n                presence_penalty: completeSettings.presencePenalty,\n                stream: true, // Enable streaming\n              }),\n            },\n          );\n\n          if (!response.ok) {\n            const errorText = await response.text();\n            throw new NetworkError(\n              `OpenRouter API error: ${response.status} ${response.statusText} - ${errorText}`,\n            );\n          }\n\n          if (!response.body) {\n            throw new Error(\"Response body is null\");\n          }\n\n          const reader = response.body.getReader();\n          const decoder = new TextDecoder(\"utf-8\");\n          let buffer = \"\";\n\n          try {\n            while (true) {\n              const { done, value } = await reader.read();\n              if (done) break;\n\n              const chunk = decoder.decode(value, { stream: true });\n              buffer += chunk;\n\n              // Process complete lines from the buffer\n              let lineEnd = buffer.indexOf(\"\\n\");\n              while (lineEnd !== -1) {\n                const line = buffer.substring(0, lineEnd).trim();\n                buffer = buffer.substring(lineEnd + 1);\n\n                if (line.startsWith(\"data: \")) {\n                  const data = line.slice(6);\n                  if (data === \"[DONE]\") continue;\n\n                  try {\n                    const parsed = JSON.parse(data);\n                    const content = parsed.choices[0]?.delta?.content || \"\";\n                    if (content) {\n                      fullResponse += content;\n                      yield {\n                        text: content,\n                        isComplete: false,\n                      };\n                    }\n                  } catch (e) {\n                    console.error(\"Error parsing streaming response:\", e);\n                  }\n                }\n\n                lineEnd = buffer.indexOf(\"\\n\");\n              }\n            }\n          } finally {\n            reader.releaseLock();\n          }\n        } catch (streamError) {\n          console.error(\n            \"[SERVER] Error in OpenRouter streaming, falling back to non-streaming:\",\n            streamError,\n          );\n          streamFailed = true;\n        }\n\n        // If streaming failed, fall back to non-streaming\n        if (streamFailed) {\n          const nonStreamingResponse = await callOpenRouterAPI(\n            completeSettings.apiKey,\n            completeSettings.model,\n            messages,\n            completeSettings.temperature,\n            completeSettings.maxTokens,\n            completeSettings.topP,\n            completeSettings.frequencyPenalty,\n            completeSettings.presencePenalty,\n          );\n\n          yield {\n            text: nonStreamingResponse,\n            isComplete: false,\n          };\n        }\n\n        yield {\n          text: \"\",\n          isComplete: true,\n        };\n      } catch (error) {\n        console.error(\n          \"[SERVER] Error in OpenRouter response generation:\",\n          error,\n        );\n        yield {\n          text: `Error: ${error instanceof Error ? error.message : String(error)}`,\n          isComplete: true,\n        };\n      }\n    } else {\n      yield {\n        text: `Error: Unsupported provider: ${completeSettings.provider}`,\n        isComplete: true,\n      };\n    }\n  } catch (error) {\n    console.error(\"[SERVER] Error in streamGenerateResponse:\", error);\n    yield {\n      text: `Error: ${error instanceof Error ? error.message : String(error)}`,\n      isComplete: true,\n    };\n  }\n}\n\n// Add response validation function\nexport async function validateResponse(\n  response: string,\n): Promise<{ valid: boolean; reason?: string }> {\n  // Basic validation to ensure response meets quality standards\n  if (!response || response.trim().length === 0) {\n    return { valid: false, reason: \"Empty response\" };\n  }\n\n  // Check for error messages that might have leaked into the response\n  if (\n    response.toLowerCase().includes(\"error\") &&\n    (response.toLowerCase().includes(\"api\") ||\n      response.toLowerCase().includes(\"key\"))\n  ) {\n    return { valid: false, reason: \"Response contains error messages\" };\n  }\n\n  // Check for minimum length (adjust as needed)\n  if (response.length < 10) {\n    return { valid: false, reason: \"Response too short\" };\n  }\n\n  return { valid: true };\n}\n\n// Enhanced implementation for extracting beliefs\nexport async function extractBeliefs(\n  conversationText: string,\n  agentName: string,\n  extractionPriorities: string,\n  settings: LLMSettings,\n): Promise<string> {\n  try {\n    logger.info(\n      \"[SERVER] extractBeliefs called with priorities:\",\n      extractionPriorities,\n    );\n\n    // Create a prompt using the belief extraction template\n    const systemPrompt = `You are an AI assistant that analyzes conversations and extracts potential new knowledge or beliefs.\nYour task is to identify information, facts, or beliefs that should be added to an agent's knowledge base.\nFocus on extracting factual information, preferences, opinions, and relationships mentioned in the conversation.\n\nIMPORTANT: Format your response using Obsidian-style markdown. Use [[double brackets]] around important concepts, entities, and categories that should be tagged.`;\n\n    const userPrompt = `The following is a conversation involving ${agentName}.\nExtract potential new knowledge or beliefs that ${agentName} should remember from this conversation.\nPay special attention to: ${extractionPriorities}\n\nCONVERSATION:\n${conversationText}\n\nList the extracted beliefs in bullet points. Each belief should be a concise statement of fact or opinion.\nFor each belief:\n1. Use [[double brackets]] around key concepts that should be tagged\n2. Indicate the confidence level (High/Medium/Low) based on how explicitly it was stated\n3. Format the belief as a complete, well-structured markdown note\n\nExample format:\n- ${agentName} believes that [[quantum computing]] will revolutionize [[cryptography]] within the next decade. (High)\n- ${agentName} seems to prefer [[coffee]] over [[tea]] based on their ordering habits. (Medium)`;\n\n    // Call the LLM service to generate a response\n    return await generateResponse(userPrompt, systemPrompt, settings);\n  } catch (error) {\n    console.error(\"[SERVER] Error in extractBeliefs:\", error);\n    throw error;\n  }\n}\n\n// Enhanced implementation for generating knowledge entries\nexport async function generateKnowledgeEntries(\n  beliefs: string,\n  settings: LLMSettings,\n): Promise<KnowledgeEntry[]> {\n  try {\n    logger.info(\"[SERVER] generateKnowledgeEntries called\");\n\n    // Parse the beliefs string to extract individual beliefs\n    const beliefLines = beliefs\n      .split(\"\\n\")\n      .filter((line) => line.trim().startsWith(\"-\"))\n      .map((line) => line.trim().substring(1).trim());\n\n    // Create knowledge entries from the beliefs\n    return beliefLines.map((belief) => {\n      // Extract tags using the existing utility\n      const tags = extractTagsFromMarkdown(belief);\n\n      // Generate a title based on the first tag or the first few words\n      const title =\n        tags.length > 0\n          ? `Knowledge about ${tags[0]}`\n          : belief.split(\" \").slice(0, 3).join(\" \");\n\n      return {\n        id: `knowledge-${Date.now()}-${Math.random().toString(36).substring(2, 7)}`,\n        title,\n        content: belief,\n        timestamp: new Date(),\n        tags,\n      };\n    });\n  } catch (error) {\n    console.error(\"[SERVER] Error in generateKnowledgeEntries:\", error);\n    return [\n      {\n        id: `error-${Date.now()}`,\n        title: \"Error\",\n        content: error instanceof Error ? error.message : \"Unknown error\",\n        timestamp: new Date(),\n        tags: [\"error\"],\n      },\n    ];\n  }\n}\n\n// Mock implementation for validating API key\nexport async function validateApiKey(\n  provider: \"openai\" | \"openrouter\",\n  apiKey: string,\n): Promise<{ valid: boolean; message?: string }> {\n  logger.info(\"[SERVER] validateApiKey called (mock implementation)\");\n  return {\n    valid: true,\n    message: `API key validation successful for ${provider}. (This is a mock)`,\n  };\n}\n\n// Mock implementation for saving LLM settings\nexport async function saveLLMSettings(settings: LLMSettings): Promise<boolean> {\n  logger.info(\"[SERVER] saveLLMSettings called\");\n  logger.info(\"[SERVER] Saving settings:\", {\n    ...settings,\n    apiKey: settings.apiKey ? `[Length: ${settings.apiKey.length}]` : undefined,\n    provider: settings.provider,\n  });\n  try {\n    // In a real app, we would save to a database here\n    // For now, we'll just return true to indicate success\n    // The client-side code will handle saving to localStorage\n    return true;\n  } catch (error) {\n    console.error(\"[SERVER] Error saving settings:\", error);\n    return false;\n  }\n}\n"],"names":["extractBeliefs","generateKnowledgeEntries","generateResponse","saveLLMSettings","streamGenerateResponse","validateApiKey","validateResponse","withRetry","logger","createLogger","info","defaultSettings","hasServerRef","keys","Object","type","operation","maxRetries","initialDelay","lastError","delay","attempt","error","Error","String","console","log","Promise","resolve","setTimeout","callOpenRouterAPI","apiKey","model","messages","temperature","max_tokens","maxTokens","top_p","topP","frequency_penalty","frequencyPenalty","presence_penalty","presencePenalty","length","substring","requestBody","JSON","stringify","fetchPromise","fetch","method","headers","Authorization","body","response","withTimeout","ok","errorText","text","status","statusText","errorData","parse","e","message","NetworkError","data","json","choices","content","userPrompt","systemPrompt","settings","debugLog","provider","undefined","completeSettings","apiKeyLength","ApiKeyError","push","role","openaiProvider","createOpenAI","result","generateText","system","prompt","LLMError","systemPromptLength","userPromptLength","settingsProvider","settingsModel","isComplete","openai","streamFailed","fullText","stream","streamText","chunk","textStream","streamError","fullResponse","reader","getReader","decoder","TextDecoder","buffer","done","value","read","decode","lineEnd","indexOf","line","trim","startsWith","slice","parsed","delta","releaseLock","nonStreamingResponse","valid","reason","toLowerCase","includes","conversationText","agentName","extractionPriorities","beliefs","beliefLines","split","filter","map","belief","tags","extractTagsFromMarkdown","title","join","id","Date","now","Math","random","toString","timestamp"],"mappings":"AAAA;;;;;;;;;;;;IA6jBsBA,cAAc;eAAdA;;IA6CAC,wBAAwB;eAAxBA;;IAtcAC,gBAAgB;eAAhBA;;IAigBAC,eAAe;eAAfA;;IAtYCC,sBAAsB;eAAtBA;;IA0XDC,cAAc;eAAdA;;IAtHAC,gBAAgB;eAAhBA;;IA5fAC,SAAS;eAATA;;;oBArCmB;wBAClB;6BAGM;uBAEW;2BAOjC;6BAC2C;AAGlD,0BAA0B;AAC1B,MAAMC,SAASC,IAAAA,yBAAY,EAAC;AAE5BD,OAAOE,IAAI,CAAC;AAQZ,gEAAgE;AAChEF,OAAOE,IAAI,CAAC,wCAAwC;IAClD,GAAGC,4BAAe;IAClBC,cAAc,kBAAkBD,4BAAe;IAC/CE,MAAMC,OAAOD,IAAI,CAACF,4BAAe;IACjCI,MAAM,OAAOJ,4BAAe;AAC9B;AAGO,eAAeJ,UACpBS,SAA2B,EAC3BC,aAAa,CAAC,EACdC,eAAe,IAAI;IAEnB,IAAIC,YAA0B;IAC9B,IAAIC,QAAQF;IAEZ,IAAK,IAAIG,UAAU,GAAGA,WAAWJ,YAAYI,UAAW;QACtD,IAAI;YACF,OAAO,MAAML;QACf,EAAE,OAAOM,OAAO;YACdH,YAAYG,iBAAiBC,QAAQD,QAAQ,IAAIC,MAAMC,OAAOF;YAC9DG,QAAQH,KAAK,CACX,CAAC,0BAA0B,EAAED,UAAU,EAAE,CAAC,EAAEJ,aAAa,EAAE,EAAE,CAAC,EAC9DE;YAGF,kCAAkC;YAClC,IAAIE,UAAUJ,YAAY;gBACxBQ,QAAQC,GAAG,CAAC,CAAC,YAAY,EAAEN,MAAM,KAAK,CAAC;gBACvC,MAAM,IAAIO,QAAQ,CAACC,UAAYC,WAAWD,SAASR;gBACnDA,SAAS,GAAG,sBAAsB;YACpC;QACF;IACF;IAEA,MAAMD,aAAa,IAAII,MAAM;AAC/B;AAEA,2CAA2C;AAC3C,eAAeO,kBACbC,MAAc,EACdC,KAAa,EACbC,QAAkD,EAClDC,cAAsBvB,4BAAe,CAACuB,WAAW,EACjDC,aAAqBxB,4BAAe,CAACyB,SAAS,EAC9CC,QAAgB1B,4BAAe,CAAC2B,IAAI,EACpCC,oBAA4B5B,4BAAe,CAAC6B,gBAAgB,EAC5DC,mBAA2B9B,4BAAe,CAAC+B,eAAe;IAE1DlC,OAAOE,IAAI,CAAC,+CAA+CsB;IAC3DxB,OAAOE,IAAI,CAAC,uCAAuCqB,OAAOY,MAAM;IAChEnC,OAAOE,IAAI,CACT,8CACAqB,OAAOa,SAAS,CAAC,GAAG;IAEtBpC,OAAOE,IAAI,CAAC,mCAAmC;QAC7CwB;QACAC;QACAE;QACAE;QACAE;IACF;IAEA,IAAI;QACF,MAAMI,cAAc;YAClBb;YACAC;YACAC;YACAC;YACAE;YACAE;YACAE;QACF;QAEAjC,OAAOE,IAAI,CAAC,0BAA0BoC,KAAKC,SAAS,CAACF;QAErD,gDAAgD;QAChD,MAAMG,eAAeC,MACnB,iDACA;YACEC,QAAQ;YACRC,SAAS;gBACP,gBAAgB;gBAChBC,eAAe,CAAC,OAAO,EAAErB,OAAO,CAAC;gBACjC,gBAAgB;gBAChB,WAAW;YACb;YACAsB,MAAMP,KAAKC,SAAS,CAACF;QACvB;QAGF,MAAMS,WAAW,MAAMC,IAAAA,sBAAW,EAACP,cAAc,OAAO;QAExD,IAAI,CAACM,SAASE,EAAE,EAAE;YAChB,MAAMC,YAAY,MAAMH,SAASI,IAAI;YACrCjC,QAAQH,KAAK,CAAC,2CAA2CmC;YACzDhC,QAAQH,KAAK,CACX,6BACAgC,SAASK,MAAM,EACfL,SAASM,UAAU;YAErBnC,QAAQH,KAAK,CAAC,6BAA6B;gBACzC,gBAAgB;gBAChB8B,eAAe,CAAC,OAAO,EAAErB,OAAOa,SAAS,CAAC,GAAG,GAAG,GAAG,CAAC;gBACpD,gBAAgB;gBAChB,WAAW;YACb;YAEA,IAAIiB;YACJ,IAAI;gBACFA,YAAYf,KAAKgB,KAAK,CAACL;YACzB,EAAE,OAAOM,GAAG;gBACVF,YAAY;oBAAEvC,OAAO;wBAAE0C,SAASP;oBAAU;gBAAE;YAC9C;YAEA,MAAM,IAAIQ,uBAAY,CACpB,CAAC,sBAAsB,EAAEX,SAASK,MAAM,CAAC,CAAC,EAAEL,SAASM,UAAU,CAAC,EAC9DC,YAAY,CAAC,GAAG,EAAEf,KAAKC,SAAS,CAACc,WAAW,CAAC,GAAG,GACjD,CAAC;QAEN;QAEA,8CAA8C;QAC9C,MAAMK,OAAO,MAAMZ,SAASa,IAAI;QAEhC,OAAOD,KAAKE,OAAO,CAAC,EAAE,CAACJ,OAAO,CAACK,OAAO;IACxC,EAAE,OAAO/C,OAAO;QACdG,QAAQH,KAAK,CAAC,0CAA0CA;QACxD,MAAMA;IACR;AACF;AAGO,eAAepB,iBACpBoE,UAAkB,EAClBC,YAAoB,EACpBC,WAAiC,CAAC,CAAC;IAEnC,IAAI;QACF,8DAA8D;QAC9DC,IAAAA,qBAAQ,EACN,CAAC,qDAAqD,EAAED,SAASE,QAAQ,CAAC,CAAC;QAE7ED,IAAAA,qBAAQ,EACN,CAAC,iCAAiC,EAAE,CAAC,CAACD,SAASzC,MAAM,CAAC,UAAU,EAAEyC,SAASzC,MAAM,EAAEY,UAAU,EAAE,CAAC;QAGlG,yBAAyB;QACzB,IAAI,CAAC6B,SAASE,QAAQ,EAAE;YACtBD,IAAAA,qBAAQ,EAAC;YACTD,SAASE,QAAQ,GAAG;QACtB;QAEA,2DAA2D;QAC3DlE,OAAOE,IAAI,CAAC,mDAAmD;YAC7D,GAAG8D,QAAQ;YACXzC,QAAQyC,SAASzC,MAAM,GACnB,CAAC,SAAS,EAAEyC,SAASzC,MAAM,CAACY,MAAM,CAAC,CAAC,CAAC,GACrCgC;YACJ/D,cAAc,kBAAkB4D;YAChC3D,MAAMC,OAAOD,IAAI,CAAC2D;QACpB;QAEA,4DAA4D;QAC5D,MAAMI,mBAAmB;YAAE,GAAGjE,4BAAe;YAAE,GAAG6D,QAAQ;QAAC;QAE3DhE,OAAOE,IAAI,CAAC,0CAA0C;YACpD,GAAGkE,gBAAgB;YACnB7C,QAAQ6C,iBAAiB7C,MAAM,GAC3B,CAAC,SAAS,EAAE6C,iBAAiB7C,MAAM,CAACY,MAAM,CAAC,CAAC,CAAC,GAC7CgC;YACJ/D,cAAc,kBAAkBgE;YAChC/D,MAAMC,OAAOD,IAAI,CAAC+D;QACpB;QAEApE,OAAOE,IAAI,CAAC,mDAAmD;YAC7DgE,UAAUE,iBAAiBF,QAAQ;YACnC1C,OAAO4C,iBAAiB5C,KAAK;YAC7BE,aAAa0C,iBAAiB1C,WAAW;YACzCE,WAAWwC,iBAAiBxC,SAAS;YACrCE,MAAMsC,iBAAiBtC,IAAI;YAC3BE,kBAAkBoC,iBAAiBpC,gBAAgB;YACnDE,iBAAiBkC,iBAAiBlC,eAAe;YACjDmC,cAAcD,iBAAiB7C,MAAM,GACjC6C,iBAAiB7C,MAAM,CAACY,MAAM,GAC9B;QACN;QAEA,gCAAgC;QAChC,IAAI,CAACiC,iBAAiB7C,MAAM,EAAE;YAC5B,MAAM,IAAI+C,sBAAW,CAACF,iBAAiBF,QAAQ;QACjD;QAEA,gDAAgD;QAChD,IAAIE,iBAAiBF,QAAQ,KAAK,cAAc;YAC9ClE,OAAOE,IAAI,CAAC;YACZ,MAAMuB,WAAqD,EAAE;YAC7D,IAAIsC,cAAc;gBAChBtC,SAAS8C,IAAI,CAAC;oBAAEC,MAAM;oBAAUX,SAASE;gBAAa;YACxD;YACAtC,SAAS8C,IAAI,CAAC;gBAAEC,MAAM;gBAAQX,SAASC;YAAW;YAElD,uCAAuC;YACvC,OAAO,MAAM/D,UACX,IACEuB,kBACE8C,iBAAiB7C,MAAM,EACvB6C,iBAAiB5C,KAAK,EACtBC,UACA2C,iBAAiB1C,WAAW,EAC5B0C,iBAAiBxC,SAAS,EAC1BwC,iBAAiBtC,IAAI,EACrBsC,iBAAiBpC,gBAAgB,EACjCoC,iBAAiBlC,eAAe,GAEpC,GACA;QAEJ,OAAO,IAAIkC,iBAAiBF,QAAQ,KAAK,UAAU;YACjDlE,OAAOE,IAAI,CAAC;YACZ,6BAA6B;YAC7B,MAAMuE,iBAAiBC,IAAAA,oBAAY,EAAC;gBAClCnD,QAAQ6C,iBAAiB7C,MAAM;YACjC;YACA,MAAMC,QAAQiD,eAAeL,iBAAiB5C,KAAK;YAEnD,iCAAiC;YACjC,MAAMmD,SAAS,MAAM5B,IAAAA,sBAAW,EAC9B6B,IAAAA,gBAAY,EAAC;gBACXpD;gBACAqD,QAAQd;gBACRe,QAAQhB;gBACRpC,aAAa0C,iBAAiB1C,WAAW;gBACzCE,WAAWwC,iBAAiBxC,SAAS;gBACrCE,MAAMsC,iBAAiBtC,IAAI;gBAC3BE,kBAAkBoC,iBAAiBpC,gBAAgB;gBACnDE,iBAAiBkC,iBAAiBlC,eAAe;YACnD,IACA,OACA;YAGF,OAAOyC,OAAOzB,IAAI;QACpB,OAAO;YACL,MAAM,IAAI6B,mBAAQ,CAChB,CAAC,sBAAsB,EAAEX,iBAAiBF,QAAQ,CAAC,CAAC,EACpD;QAEJ;IACF,EAAE,OAAOpD,OAAO;QACdd,OAAOc,KAAK,CAAC,uCAAuCA;QACpD,MAAMA;IACR;AACF;AAGO,gBAAgBlB,uBACrBmE,YAAoB,EACpBD,UAAkB,EAClBE,QAAqB;IAErB,IAAI;QACFhE,OAAOE,IAAI,CAAC;QACZF,OAAOE,IAAI,CAAC,+CAA+C;YACzD8E,oBAAoBjB,cAAc5B;YAClC8C,kBAAkBnB,YAAY3B;YAC9B+C,kBAAkBlB,UAAUE;YAC5BiB,eAAenB,UAAUxC;QAC3B;QAEA,4DAA4D;QAC5D,MAAM4C,mBAAmB;YAAE,GAAGjE,4BAAe;YAAE,GAAG6D,QAAQ;QAAC;QAE3DhE,OAAOE,IAAI,CAAC,yDAAyD;YACnEgE,UAAUE,iBAAiBF,QAAQ;YACnC1C,OAAO4C,iBAAiB5C,KAAK;YAC7BE,aAAa0C,iBAAiB1C,WAAW;YACzC2C,cAAcD,iBAAiB7C,MAAM,GACjC6C,iBAAiB7C,MAAM,CAACY,MAAM,GAC9B;QACN;QAEA,0BAA0B;QAC1B,mFAAmF;QACnF,iDAAiD;QACjD,0BAA0B;QAC1B,wBAAwB;QACxB,2BAA2B;QAC3B,oDAAoD;QACpD,UAAU;QACV,mEAAmE;QACnE,mEAAmE;QAEnE,6EAA6E;QAC7E,6CAA6C;QAC7C,uCAAuC;QACvC,mDAAmD;QACnD,oFAAoF;QACpF,SAAS;QAET,gCAAgC;QAChC,IAAI,CAACiC,iBAAiB7C,MAAM,EAAE;YAC5B,MAAM;gBACJ2B,MAAM,CAAC,+BAA+B,EAAEkB,iBAAiBF,QAAQ,CAAC,SAAS,CAAC;gBAC5EkB,YAAY;YACd;YACA;QACF;QAEA,IAAIhB,iBAAiBF,QAAQ,KAAK,UAAU;YAC1ClE,OAAOE,IAAI,CAAC;YAEZ,IAAI;gBACF,MAAMsB,QAAQ6D,IAAAA,cAAM,EAACjB,iBAAiB5C,KAAK;gBAE3C,mDAAmD;gBACnD,IAAI8D,eAAe;gBACnB,IAAIC,WAAW;gBAEf,IAAI;oBACF,MAAMC,SAAS,MAAMC,IAAAA,cAAU,EAAC;wBAC9BjE;wBACAqD,QAAQd;wBACRe,QAAQhB;oBACV;oBAEA,WAAW,MAAM4B,SAASF,OAAOG,UAAU,CAAE;wBAC3CJ,YAAYG;wBACZ,MAAM;4BACJxC,MAAMwC;4BACNN,YAAY;wBACd;oBACF;gBACF,EAAE,OAAOQ,aAAa;oBACpB3E,QAAQH,KAAK,CACX,sEACA8E;oBAEFN,eAAe;gBACjB;gBAEA,kDAAkD;gBAClD,IAAIA,cAAc;oBAChB,MAAM,EAAEpC,IAAI,EAAE,GAAG,MAAM0B,IAAAA,gBAAY,EAAC;wBAClCpD;wBACAqD,QAAQd;wBACRe,QAAQhB;oBACV;oBAEA,MAAM;wBACJZ;wBACAkC,YAAY;oBACd;gBACF;gBAEA,MAAM;oBACJlC,MAAM;oBACNkC,YAAY;gBACd;YACF,EAAE,OAAOtE,OAAO;gBACdG,QAAQH,KAAK,CAAC,iDAAiDA;gBAC/D,MAAM;oBACJoC,MAAM,CAAC,OAAO,EAAEpC,iBAAiBC,QAAQD,MAAM0C,OAAO,GAAGxC,OAAOF,OAAO,CAAC;oBACxEsE,YAAY;gBACd;YACF;QACF,OAAO,IAAIhB,iBAAiBF,QAAQ,KAAK,cAAc;YACrD,sDAAsD;YACtDlE,OAAOE,IAAI,CAAC;YAEZ,IAAI;gBACF,MAAMuB,WAAW,EAAE;gBACnB,IAAIsC,cAAc;oBAChBtC,SAAS8C,IAAI,CAAC;wBAAEC,MAAM;wBAAUX,SAASE;oBAAa;gBACxD;gBACAtC,SAAS8C,IAAI,CAAC;oBAAEC,MAAM;oBAAQX,SAASC;gBAAW;gBAElD,sBAAsB;gBACtB,IAAIwB,eAAe;gBACnB,IAAIO,eAAe;gBAEnB,IAAI;oBACF,MAAM/C,WAAW,MAAML,MACrB,iDACA;wBACEC,QAAQ;wBACRC,SAAS;4BACP,gBAAgB;4BAChBC,eAAe,CAAC,OAAO,EAAEwB,iBAAiB7C,MAAM,CAAC,CAAC;4BAClD,gBAAgB;4BAChB,WAAW;wBACb;wBACAsB,MAAMP,KAAKC,SAAS,CAAC;4BACnBf,OAAO4C,iBAAiB5C,KAAK;4BAC7BC;4BACAC,aAAa0C,iBAAiB1C,WAAW;4BACzCC,YAAYyC,iBAAiBxC,SAAS;4BACtCC,OAAOuC,iBAAiBtC,IAAI;4BAC5BC,mBAAmBqC,iBAAiBpC,gBAAgB;4BACpDC,kBAAkBmC,iBAAiBlC,eAAe;4BAClDsD,QAAQ;wBACV;oBACF;oBAGF,IAAI,CAAC1C,SAASE,EAAE,EAAE;wBAChB,MAAMC,YAAY,MAAMH,SAASI,IAAI;wBACrC,MAAM,IAAIO,uBAAY,CACpB,CAAC,sBAAsB,EAAEX,SAASK,MAAM,CAAC,CAAC,EAAEL,SAASM,UAAU,CAAC,GAAG,EAAEH,UAAU,CAAC;oBAEpF;oBAEA,IAAI,CAACH,SAASD,IAAI,EAAE;wBAClB,MAAM,IAAI9B,MAAM;oBAClB;oBAEA,MAAM+E,SAAShD,SAASD,IAAI,CAACkD,SAAS;oBACtC,MAAMC,UAAU,IAAIC,YAAY;oBAChC,IAAIC,SAAS;oBAEb,IAAI;wBACF,MAAO,KAAM;4BACX,MAAM,EAAEC,IAAI,EAAEC,KAAK,EAAE,GAAG,MAAMN,OAAOO,IAAI;4BACzC,IAAIF,MAAM;4BAEV,MAAMT,QAAQM,QAAQM,MAAM,CAACF,OAAO;gCAAEZ,QAAQ;4BAAK;4BACnDU,UAAUR;4BAEV,yCAAyC;4BACzC,IAAIa,UAAUL,OAAOM,OAAO,CAAC;4BAC7B,MAAOD,YAAY,CAAC,EAAG;gCACrB,MAAME,OAAOP,OAAO9D,SAAS,CAAC,GAAGmE,SAASG,IAAI;gCAC9CR,SAASA,OAAO9D,SAAS,CAACmE,UAAU;gCAEpC,IAAIE,KAAKE,UAAU,CAAC,WAAW;oCAC7B,MAAMjD,OAAO+C,KAAKG,KAAK,CAAC;oCACxB,IAAIlD,SAAS,UAAU;oCAEvB,IAAI;wCACF,MAAMmD,SAASvE,KAAKgB,KAAK,CAACI;wCAC1B,MAAMG,UAAUgD,OAAOjD,OAAO,CAAC,EAAE,EAAEkD,OAAOjD,WAAW;wCACrD,IAAIA,SAAS;4CACXgC,gBAAgBhC;4CAChB,MAAM;gDACJX,MAAMW;gDACNuB,YAAY;4CACd;wCACF;oCACF,EAAE,OAAO7B,GAAG;wCACVtC,QAAQH,KAAK,CAAC,qCAAqCyC;oCACrD;gCACF;gCAEAgD,UAAUL,OAAOM,OAAO,CAAC;4BAC3B;wBACF;oBACF,SAAU;wBACRV,OAAOiB,WAAW;oBACpB;gBACF,EAAE,OAAOnB,aAAa;oBACpB3E,QAAQH,KAAK,CACX,0EACA8E;oBAEFN,eAAe;gBACjB;gBAEA,kDAAkD;gBAClD,IAAIA,cAAc;oBAChB,MAAM0B,uBAAuB,MAAM1F,kBACjC8C,iBAAiB7C,MAAM,EACvB6C,iBAAiB5C,KAAK,EACtBC,UACA2C,iBAAiB1C,WAAW,EAC5B0C,iBAAiBxC,SAAS,EAC1BwC,iBAAiBtC,IAAI,EACrBsC,iBAAiBpC,gBAAgB,EACjCoC,iBAAiBlC,eAAe;oBAGlC,MAAM;wBACJgB,MAAM8D;wBACN5B,YAAY;oBACd;gBACF;gBAEA,MAAM;oBACJlC,MAAM;oBACNkC,YAAY;gBACd;YACF,EAAE,OAAOtE,OAAO;gBACdG,QAAQH,KAAK,CACX,qDACAA;gBAEF,MAAM;oBACJoC,MAAM,CAAC,OAAO,EAAEpC,iBAAiBC,QAAQD,MAAM0C,OAAO,GAAGxC,OAAOF,OAAO,CAAC;oBACxEsE,YAAY;gBACd;YACF;QACF,OAAO;YACL,MAAM;gBACJlC,MAAM,CAAC,6BAA6B,EAAEkB,iBAAiBF,QAAQ,CAAC,CAAC;gBACjEkB,YAAY;YACd;QACF;IACF,EAAE,OAAOtE,OAAO;QACdG,QAAQH,KAAK,CAAC,6CAA6CA;QAC3D,MAAM;YACJoC,MAAM,CAAC,OAAO,EAAEpC,iBAAiBC,QAAQD,MAAM0C,OAAO,GAAGxC,OAAOF,OAAO,CAAC;YACxEsE,YAAY;QACd;IACF;AACF;AAGO,eAAetF,iBACpBgD,QAAgB;IAEhB,8DAA8D;IAC9D,IAAI,CAACA,YAAYA,SAAS4D,IAAI,GAAGvE,MAAM,KAAK,GAAG;QAC7C,OAAO;YAAE8E,OAAO;YAAOC,QAAQ;QAAiB;IAClD;IAEA,oEAAoE;IACpE,IACEpE,SAASqE,WAAW,GAAGC,QAAQ,CAAC,YAC/BtE,CAAAA,SAASqE,WAAW,GAAGC,QAAQ,CAAC,UAC/BtE,SAASqE,WAAW,GAAGC,QAAQ,CAAC,MAAK,GACvC;QACA,OAAO;YAAEH,OAAO;YAAOC,QAAQ;QAAmC;IACpE;IAEA,8CAA8C;IAC9C,IAAIpE,SAASX,MAAM,GAAG,IAAI;QACxB,OAAO;YAAE8E,OAAO;YAAOC,QAAQ;QAAqB;IACtD;IAEA,OAAO;QAAED,OAAO;IAAK;AACvB;AAGO,eAAezH,eACpB6H,gBAAwB,EACxBC,SAAiB,EACjBC,oBAA4B,EAC5BvD,QAAqB;IAErB,IAAI;QACFhE,OAAOE,IAAI,CACT,mDACAqH;QAGF,uDAAuD;QACvD,MAAMxD,eAAe,CAAC;;;;iKAIuI,CAAC;QAE9J,MAAMD,aAAa,CAAC,0CAA0C,EAAEwD,UAAU;gDAC9B,EAAEA,UAAU;0BAClC,EAAEC,qBAAqB;;;AAGjD,EAAEF,iBAAiB;;;;;;;;;EASjB,EAAEC,UAAU;EACZ,EAAEA,UAAU,iFAAiF,CAAC;QAE5F,8CAA8C;QAC9C,OAAO,MAAM5H,iBAAiBoE,YAAYC,cAAcC;IAC1D,EAAE,OAAOlD,OAAO;QACdG,QAAQH,KAAK,CAAC,qCAAqCA;QACnD,MAAMA;IACR;AACF;AAGO,eAAerB,yBACpB+H,OAAe,EACfxD,QAAqB;IAErB,IAAI;QACFhE,OAAOE,IAAI,CAAC;QAEZ,yDAAyD;QACzD,MAAMuH,cAAcD,QACjBE,KAAK,CAAC,MACNC,MAAM,CAAC,CAAClB,OAASA,KAAKC,IAAI,GAAGC,UAAU,CAAC,MACxCiB,GAAG,CAAC,CAACnB,OAASA,KAAKC,IAAI,GAAGtE,SAAS,CAAC,GAAGsE,IAAI;QAE9C,4CAA4C;QAC5C,OAAOe,YAAYG,GAAG,CAAC,CAACC;YACtB,0CAA0C;YAC1C,MAAMC,OAAOC,IAAAA,8BAAuB,EAACF;YAErC,iEAAiE;YACjE,MAAMG,QACJF,KAAK3F,MAAM,GAAG,IACV,CAAC,gBAAgB,EAAE2F,IAAI,CAAC,EAAE,CAAC,CAAC,GAC5BD,OAAOH,KAAK,CAAC,KAAKd,KAAK,CAAC,GAAG,GAAGqB,IAAI,CAAC;YAEzC,OAAO;gBACLC,IAAI,CAAC,UAAU,EAAEC,KAAKC,GAAG,GAAG,CAAC,EAAEC,KAAKC,MAAM,GAAGC,QAAQ,CAAC,IAAInG,SAAS,CAAC,GAAG,GAAG,CAAC;gBAC3E4F;gBACAnE,SAASgE;gBACTW,WAAW,IAAIL;gBACfL;YACF;QACF;IACF,EAAE,OAAOhH,OAAO;QACdG,QAAQH,KAAK,CAAC,+CAA+CA;QAC7D,OAAO;YACL;gBACEoH,IAAI,CAAC,MAAM,EAAEC,KAAKC,GAAG,GAAG,CAAC;gBACzBJ,OAAO;gBACPnE,SAAS/C,iBAAiBC,QAAQD,MAAM0C,OAAO,GAAG;gBAClDgF,WAAW,IAAIL;gBACfL,MAAM;oBAAC;iBAAQ;YACjB;SACD;IACH;AACF;AAGO,eAAejI,eACpBqE,QAAiC,EACjC3C,MAAc;IAEdvB,OAAOE,IAAI,CAAC;IACZ,OAAO;QACL+G,OAAO;QACPzD,SAAS,CAAC,kCAAkC,EAAEU,SAAS,kBAAkB,CAAC;IAC5E;AACF;AAGO,eAAevE,gBAAgBqE,QAAqB;IACzDhE,OAAOE,IAAI,CAAC;IACZF,OAAOE,IAAI,CAAC,6BAA6B;QACvC,GAAG8D,QAAQ;QACXzC,QAAQyC,SAASzC,MAAM,GAAG,CAAC,SAAS,EAAEyC,SAASzC,MAAM,CAACY,MAAM,CAAC,CAAC,CAAC,GAAGgC;QAClED,UAAUF,SAASE,QAAQ;IAC7B;IACA,IAAI;QACF,kDAAkD;QAClD,sDAAsD;QACtD,0DAA0D;QAC1D,OAAO;IACT,EAAE,OAAOpD,OAAO;QACdG,QAAQH,KAAK,CAAC,mCAAmCA;QACjD,OAAO;IACT;AACF"}