{"version":3,"sources":["/Users/matthewmoroney/builds/FreeAgentics/web/__tests__/lib/active-inference.test.ts"],"sourcesContent":["/**\n * Active Inference Tests\n *\n * Tests for active inference, free energy minimization, and belief updating\n * following ADR-007 comprehensive testing requirements.\n */\n\nimport {\n  ActiveInferenceEngine,\n  createActiveInferenceEngine,\n  performInference,\n  selectAction,\n  updateBeliefs,\n  calculateExpectedFreeEnergy,\n  calculateEpistemic,\n  calculatePragmatic,\n  minimizeVariationalFreeEnergy,\n  predictSensoryOutcomes,\n  evaluateActionPolicy,\n  GenerativeModel,\n  Beliefs,\n  SensoryInput,\n  Action,\n  Policy,\n} from \"@/lib/active-inference\";\n\ndescribe(\"Active Inference Engine\", () => {\n  let engine: ActiveInferenceEngine;\n  let testModel: GenerativeModel;\n\n  beforeEach(() => {\n    testModel = {\n      states: [\"exploring\", \"exploiting\", \"resting\"],\n      observations: [\"high_reward\", \"low_reward\", \"no_reward\"],\n      actions: [\"move_forward\", \"turn\", \"wait\"],\n\n      // Transition dynamics P(s'|s,a)\n      transitionModel: {\n        exploring: {\n          move_forward: { exploring: 0.7, exploiting: 0.2, resting: 0.1 },\n          turn: { exploring: 0.8, exploiting: 0.1, resting: 0.1 },\n          wait: { exploring: 0.3, exploiting: 0.1, resting: 0.6 },\n        },\n        exploiting: {\n          move_forward: { exploring: 0.1, exploiting: 0.8, resting: 0.1 },\n          turn: { exploring: 0.3, exploiting: 0.6, resting: 0.1 },\n          wait: { exploring: 0.1, exploiting: 0.3, resting: 0.6 },\n        },\n        resting: {\n          move_forward: { exploring: 0.6, exploiting: 0.3, resting: 0.1 },\n          turn: { exploring: 0.4, exploiting: 0.2, resting: 0.4 },\n          wait: { exploring: 0.1, exploiting: 0.1, resting: 0.8 },\n        },\n      },\n\n      // Observation model P(o|s)\n      observationModel: {\n        exploring: { high_reward: 0.3, low_reward: 0.5, no_reward: 0.2 },\n        exploiting: { high_reward: 0.7, low_reward: 0.2, no_reward: 0.1 },\n        resting: { high_reward: 0.1, low_reward: 0.2, no_reward: 0.7 },\n      },\n\n      // Prior preferences (negative log probabilities)\n      preferences: {\n        high_reward: -2.0, // Strongly preferred\n        low_reward: -0.5, // Mildly preferred\n        no_reward: 0.5, // Slightly dispreferred\n      },\n    };\n\n    engine = createActiveInferenceEngine({\n      model: testModel,\n      precision: 1.0,\n      learningRate: 0.1,\n      planningHorizon: 3,\n    });\n  });\n\n  describe(\"Engine Creation and Configuration\", () => {\n    it(\"creates engine with valid configuration\", () => {\n      expect(engine).toMatchObject({\n        model: testModel,\n        precision: 1.0,\n        learningRate: 0.1,\n        planningHorizon: 3,\n      });\n    });\n\n    it(\"initializes with uniform beliefs\", () => {\n      const beliefs = engine.getCurrentBeliefs();\n      const stateProbs = Object.values(beliefs.states);\n\n      expect(\n        Math.abs(stateProbs.reduce((sum, p) => sum + p, 0) - 1.0),\n      ).toBeLessThan(0.001);\n      stateProbs.forEach((p) => {\n        expect(p).toBeCloseTo(1.0 / testModel.states.length, 5);\n      });\n    });\n\n    it(\"validates model structure\", () => {\n      const invalidModel = {\n        ...testModel,\n        transitionModel: {}, // Invalid: empty transitions\n      };\n\n      expect(() => {\n        createActiveInferenceEngine({ model: invalidModel });\n      }).toThrow(\"Invalid generative model\");\n    });\n  });\n\n  describe(\"Belief Updates\", () => {\n    it(\"updates beliefs based on observations\", () => {\n      const initialBeliefs = engine.getCurrentBeliefs();\n      const observation: SensoryInput = {\n        type: \"observation\",\n        value: \"high_reward\",\n        confidence: 0.9,\n      };\n\n      const updatedBeliefs = updateBeliefs(engine, observation);\n\n      // Should increase belief in exploiting state (high reward association)\n      expect(updatedBeliefs.states.exploiting).toBeGreaterThan(\n        initialBeliefs.states.exploiting,\n      );\n    });\n\n    it(\"incorporates observation confidence\", () => {\n      const highConfObs: SensoryInput = {\n        type: \"observation\",\n        value: \"high_reward\",\n        confidence: 0.95,\n      };\n\n      const lowConfObs: SensoryInput = {\n        type: \"observation\",\n        value: \"high_reward\",\n        confidence: 0.3,\n      };\n\n      const highConfBeliefs = updateBeliefs(engine, highConfObs);\n      const lowConfBeliefs = updateBeliefs(engine, lowConfObs);\n\n      // High confidence should lead to stronger belief updates\n      const highConfChange = Math.abs(\n        highConfBeliefs.states.exploiting - 1 / 3,\n      );\n      const lowConfChange = Math.abs(lowConfBeliefs.states.exploiting - 1 / 3);\n\n      expect(highConfChange).toBeGreaterThan(lowConfChange);\n    });\n\n    it(\"maintains probability normalization\", () => {\n      const observations = [\n        { type: \"observation\", value: \"high_reward\", confidence: 0.8 },\n        { type: \"observation\", value: \"low_reward\", confidence: 0.7 },\n        { type: \"observation\", value: \"no_reward\", confidence: 0.9 },\n      ];\n\n      observations.forEach((obs) => {\n        const beliefs = updateBeliefs(engine, obs as SensoryInput);\n        const sum = Object.values(beliefs.states).reduce((s, p) => s + p, 0);\n        expect(sum).toBeCloseTo(1.0, 5);\n      });\n    });\n  });\n\n  describe(\"Free Energy Calculations\", () => {\n    it(\"calculates variational free energy\", () => {\n      const beliefs: Beliefs = {\n        states: { exploring: 0.6, exploiting: 0.3, resting: 0.1 },\n        uncertainty: 0.2,\n      };\n\n      const observation = \"high_reward\";\n      const freeEnergy = engine.calculateFreeEnergy(beliefs, observation);\n\n      expect(freeEnergy).toBeGreaterThan(0);\n      expect(Number.isFinite(freeEnergy)).toBe(true);\n    });\n\n    it(\"increases with prediction error\", () => {\n      // Belief state that predicts low rewards\n      const pessimisticBeliefs: Beliefs = {\n        states: { exploring: 0.1, exploiting: 0.1, resting: 0.8 },\n        uncertainty: 0.1,\n      };\n\n      // But observes high reward\n      const observation = \"high_reward\";\n\n      const highError = engine.calculateFreeEnergy(\n        pessimisticBeliefs,\n        observation,\n      );\n\n      // Belief state that predicts high rewards\n      const optimisticBeliefs: Beliefs = {\n        states: { exploring: 0.1, exploiting: 0.8, resting: 0.1 },\n        uncertainty: 0.1,\n      };\n\n      const lowError = engine.calculateFreeEnergy(\n        optimisticBeliefs,\n        observation,\n      );\n\n      expect(highError).toBeGreaterThan(lowError);\n    });\n\n    it(\"includes entropy/uncertainty term\", () => {\n      const certainBeliefs: Beliefs = {\n        states: { exploring: 0.98, exploiting: 0.01, resting: 0.01 },\n        uncertainty: 0.01,\n      };\n\n      const uncertainBeliefs: Beliefs = {\n        states: { exploring: 0.34, exploiting: 0.33, resting: 0.33 },\n        uncertainty: 0.8,\n      };\n\n      const observation = \"low_reward\";\n      const certainFE = engine.calculateFreeEnergy(certainBeliefs, observation);\n      const uncertainFE = engine.calculateFreeEnergy(\n        uncertainBeliefs,\n        observation,\n      );\n\n      // Higher uncertainty should contribute to free energy\n      expect(Math.abs(certainFE - uncertainFE)).toBeGreaterThan(0.1);\n    });\n  });\n\n  describe(\"Expected Free Energy\", () => {\n    it(\"calculates expected free energy for policies\", () => {\n      const policy: Policy = [\n        { action: \"move_forward\", timestep: 0 },\n        { action: \"turn\", timestep: 1 },\n        { action: \"wait\", timestep: 2 },\n      ];\n\n      const efe = calculateExpectedFreeEnergy(engine, policy);\n\n      expect(Number.isFinite(efe.total)).toBe(true);\n      expect(Number.isFinite(efe.epistemic)).toBe(true);\n      expect(Number.isFinite(efe.pragmatic)).toBe(true);\n      expect(efe).toHaveProperty(\"total\");\n      expect(efe).toHaveProperty(\"epistemic\");\n      expect(efe).toHaveProperty(\"pragmatic\");\n    });\n\n    it(\"balances epistemic and pragmatic value\", () => {\n      const exploratoryPolicy: Policy = [\n        { action: \"turn\", timestep: 0 },\n        { action: \"move_forward\", timestep: 1 },\n      ];\n\n      const exploitativePolicy: Policy = [\n        { action: \"wait\", timestep: 0 },\n        { action: \"wait\", timestep: 1 },\n      ];\n\n      const exploratory = calculateExpectedFreeEnergy(\n        engine,\n        exploratoryPolicy,\n      );\n      const exploitative = calculateExpectedFreeEnergy(\n        engine,\n        exploitativePolicy,\n      );\n\n      expect(exploratory.epistemic).toBeLessThan(exploitative.epistemic);\n    });\n\n    it(\"considers future outcomes\", () => {\n      const shortPolicy: Policy = [{ action: \"move_forward\", timestep: 0 }];\n\n      const longPolicy: Policy = [\n        { action: \"move_forward\", timestep: 0 },\n        { action: \"move_forward\", timestep: 1 },\n        { action: \"move_forward\", timestep: 2 },\n      ];\n\n      const shortEFE = calculateExpectedFreeEnergy(engine, shortPolicy);\n      const longEFE = calculateExpectedFreeEnergy(engine, longPolicy);\n\n      // Longer horizon should affect total expected free energy\n      expect(shortEFE.total).not.toBe(longEFE.total);\n    });\n  });\n\n  describe(\"Action Selection\", () => {\n    it(\"selects actions that minimize expected free energy\", () => {\n      const currentBeliefs: Beliefs = {\n        states: { exploring: 0.7, exploiting: 0.2, resting: 0.1 },\n        uncertainty: 0.3,\n      };\n\n      const selectedAction = selectAction(engine, currentBeliefs);\n\n      expect(testModel.actions).toContain(selectedAction.type);\n      expect(selectedAction.confidence).toBeGreaterThan(0);\n      expect(selectedAction.confidence).toBeLessThanOrEqual(1);\n    });\n\n    it(\"increases exploration under high uncertainty\", () => {\n      const lowUncertainty: Beliefs = {\n        states: { exploring: 0.1, exploiting: 0.8, resting: 0.1 },\n        uncertainty: 0.1,\n      };\n\n      const highUncertainty: Beliefs = {\n        states: { exploring: 0.1, exploiting: 0.8, resting: 0.1 },\n        uncertainty: 0.8,\n      };\n\n      const lowUncAction = selectAction(engine, lowUncertainty);\n      const highUncAction = selectAction(engine, highUncertainty);\n\n      // High uncertainty should favor exploratory actions\n      expect(highUncAction.type).not.toBe(\"wait\");\n    });\n\n    it(\"respects action constraints\", () => {\n      const constrainedEngine = createActiveInferenceEngine({\n        model: testModel,\n        actionConstraints: {\n          wait: { maxFrequency: 0.3 },\n        },\n      });\n\n      const actions = [];\n      for (let i = 0; i < 100; i++) {\n        const action = selectAction(\n          constrainedEngine,\n          engine.getCurrentBeliefs(),\n        );\n        actions.push(action.type);\n      }\n\n      const waitFrequency =\n        actions.filter((a) => a === \"wait\").length / actions.length;\n      expect(waitFrequency).toBeLessThanOrEqual(0.35); // Allow small variance\n    });\n  });\n\n  describe(\"Inference Process\", () => {\n    it(\"performs complete inference cycle\", async () => {\n      const observation: SensoryInput = {\n        type: \"observation\",\n        value: \"high_reward\",\n        confidence: 0.9,\n      };\n\n      const result = await performInference(engine, observation);\n\n      expect(result).toHaveProperty(\"beliefs\");\n      expect(result).toHaveProperty(\"selectedAction\");\n      expect(result).toHaveProperty(\"freeEnergy\");\n      expect(result).toHaveProperty(\"confidence\");\n    });\n\n    it(\"adapts to changing observations\", async () => {\n      const observations = [\n        { type: \"observation\", value: \"no_reward\", confidence: 0.9 },\n        { type: \"observation\", value: \"low_reward\", confidence: 0.8 },\n        { type: \"observation\", value: \"high_reward\", confidence: 0.95 },\n      ];\n\n      const results = [];\n      for (const obs of observations) {\n        const result = await performInference(engine, obs as SensoryInput);\n        results.push(result);\n      }\n\n      // Should adapt behavior based on observation history\n      expect(results[0].selectedAction.type).not.toBe(\n        results[2].selectedAction.type,\n      );\n    });\n\n    it(\"maintains computational efficiency\", async () => {\n      const startTime = Date.now();\n      const iterations = 100;\n\n      for (let i = 0; i < iterations; i++) {\n        const obs: SensoryInput = {\n          type: \"observation\",\n          value: [\"high_reward\", \"low_reward\", \"no_reward\"][i % 3],\n          confidence: 0.8,\n        };\n        await performInference(engine, obs);\n      }\n\n      const totalTime = Date.now() - startTime;\n      const avgTime = totalTime / iterations;\n\n      expect(avgTime).toBeLessThan(10); // Should be fast (< 10ms per inference)\n    });\n  });\n\n  describe(\"Epistemic and Pragmatic Value\", () => {\n    it(\"calculates epistemic value (information gain)\", () => {\n      const beliefs: Beliefs = {\n        states: { exploring: 0.5, exploiting: 0.3, resting: 0.2 },\n        uncertainty: 0.4,\n      };\n\n      const action = \"move_forward\";\n      const epistemicValue = calculateEpistemic(engine, beliefs, action);\n\n      expect(epistemicValue).toBeGreaterThanOrEqual(0);\n      expect(Number.isFinite(epistemicValue)).toBe(true);\n    });\n\n    it(\"calculates pragmatic value (goal achievement)\", () => {\n      const beliefs: Beliefs = {\n        states: { exploring: 0.2, exploiting: 0.7, resting: 0.1 },\n        uncertainty: 0.2,\n      };\n\n      const action = \"move_forward\";\n      const pragmaticValue = calculatePragmatic(engine, beliefs, action);\n\n      expect(Number.isFinite(pragmaticValue)).toBe(true);\n      // Should be negative (lower is better) for good actions\n    });\n\n    it(\"trades off exploration vs exploitation\", () => {\n      const beliefs: Beliefs = {\n        states: { exploring: 0.4, exploiting: 0.4, resting: 0.2 },\n        uncertainty: 0.5,\n      };\n\n      const explore = calculateEpistemic(engine, beliefs, \"turn\");\n      const exploit = calculatePragmatic(engine, beliefs, \"wait\");\n\n      // Both values should influence decision\n      expect(explore).toBeGreaterThan(0);\n      expect(Math.abs(exploit)).toBeGreaterThan(0);\n    });\n  });\n\n  describe(\"Variational Free Energy Minimization\", () => {\n    it(\"minimizes free energy through gradient descent\", async () => {\n      const initialBeliefs: Beliefs = {\n        states: { exploring: 0.6, exploiting: 0.2, resting: 0.2 },\n        uncertainty: 0.5,\n      };\n\n      const observation = \"high_reward\";\n      const initialFE = engine.calculateFreeEnergy(initialBeliefs, observation);\n\n      const optimized = await minimizeVariationalFreeEnergy(\n        engine,\n        initialBeliefs,\n        observation,\n        { maxIterations: 50 },\n      );\n\n      const finalFE = engine.calculateFreeEnergy(optimized, observation);\n\n      expect(finalFE).toBeLessThan(initialFE);\n    });\n\n    it(\"converges to stable solution\", async () => {\n      const beliefs: Beliefs = {\n        states: { exploring: 0.33, exploiting: 0.33, resting: 0.34 },\n        uncertainty: 0.3,\n      };\n\n      const observation = \"low_reward\";\n      const optimized = await minimizeVariationalFreeEnergy(\n        engine,\n        beliefs,\n        observation,\n        { maxIterations: 100, tolerance: 0.001 },\n      );\n\n      // Should converge to stable beliefs\n      expect(optimized.converged).toBe(true);\n      expect(optimized.iterations).toBeLessThan(100);\n    });\n  });\n\n  describe(\"Predictive Processing\", () => {\n    it(\"predicts future sensory outcomes\", () => {\n      const currentBeliefs: Beliefs = {\n        states: { exploring: 0.7, exploiting: 0.2, resting: 0.1 },\n        uncertainty: 0.2,\n      };\n\n      const action = \"move_forward\";\n      const predictions = predictSensoryOutcomes(\n        engine,\n        currentBeliefs,\n        action,\n      );\n\n      expect(predictions).toHaveProperty(\"high_reward\");\n      expect(predictions).toHaveProperty(\"low_reward\");\n      expect(predictions).toHaveProperty(\"no_reward\");\n\n      const totalProb = Object.values(predictions).reduce(\n        (sum, p) => sum + p,\n        0,\n      );\n      expect(totalProb).toBeCloseTo(1.0, 5);\n    });\n\n    it(\"updates predictions based on actions\", () => {\n      const beliefs: Beliefs = {\n        states: { exploring: 0.1, exploiting: 0.8, resting: 0.1 },\n        uncertainty: 0.1,\n      };\n\n      const movePredict = predictSensoryOutcomes(\n        engine,\n        beliefs,\n        \"move_forward\",\n      );\n      const waitPredict = predictSensoryOutcomes(engine, beliefs, \"wait\");\n\n      // Different actions should lead to different predictions\n      expect(movePredict.high_reward).not.toBe(waitPredict.high_reward);\n    });\n  });\n\n  describe(\"Policy Evaluation\", () => {\n    it(\"evaluates action sequences\", () => {\n      const policy: Policy = [\n        { action: \"move_forward\", timestep: 0 },\n        { action: \"move_forward\", timestep: 1 },\n        { action: \"turn\", timestep: 2 },\n      ];\n\n      const evaluation = evaluateActionPolicy(engine, policy);\n\n      expect(evaluation).toHaveProperty(\"expectedReturn\");\n      expect(evaluation).toHaveProperty(\"uncertainty\");\n      expect(evaluation).toHaveProperty(\"feasibility\");\n    });\n\n    it(\"compares alternative policies\", () => {\n      const greedyPolicy: Policy = [\n        { action: \"wait\", timestep: 0 },\n        { action: \"wait\", timestep: 1 },\n      ];\n\n      const balancedPolicy: Policy = [\n        { action: \"move_forward\", timestep: 0 },\n        { action: \"turn\", timestep: 1 },\n      ];\n\n      const greedyEval = evaluateActionPolicy(engine, greedyPolicy);\n      const balancedEval = evaluateActionPolicy(engine, balancedPolicy);\n\n      // Policies should have different characteristics\n      expect(greedyEval.uncertainty).not.toBe(balancedEval.uncertainty);\n    });\n  });\n\n  describe(\"Integration Tests\", () => {\n    it(\"handles continuous operation\", async () => {\n      const session = {\n        observations: 0,\n        actions: [],\n        beliefs: [],\n        freeEnergies: [],\n      };\n\n      // Simulate extended interaction\n      for (let t = 0; t < 20; t++) {\n        const obs: SensoryInput = {\n          type: \"observation\",\n          value: Math.random() > 0.5 ? \"high_reward\" : \"low_reward\",\n          confidence: 0.8 + Math.random() * 0.2,\n        };\n\n        const result = await performInference(engine, obs);\n\n        session.observations++;\n        session.actions.push(result.selectedAction.type);\n        session.beliefs.push(result.beliefs);\n        session.freeEnergies.push(result.freeEnergy);\n      }\n\n      // Should show adaptive behavior\n      const uniqueActions = new Set(session.actions).size;\n      expect(uniqueActions).toBeGreaterThan(1);\n\n      // Free energy should generally decrease\n      const avgEarlyFE =\n        session.freeEnergies.slice(0, 5).reduce((a, b) => a + b) / 5;\n      const avgLateFE =\n        session.freeEnergies.slice(-5).reduce((a, b) => a + b) / 5;\n      expect(avgLateFE).toBeLessThanOrEqual(avgEarlyFE);\n    });\n\n    it(\"recovers from unexpected observations\", async () => {\n      // Set strong beliefs\n      engine.setBeliefs({\n        states: { exploring: 0.05, exploiting: 0.9, resting: 0.05 },\n        uncertainty: 0.1,\n      });\n\n      // Unexpected observation\n      const surprise: SensoryInput = {\n        type: \"observation\",\n        value: \"no_reward\",\n        confidence: 0.95,\n      };\n\n      const beforeFE = engine.calculateFreeEnergy(\n        engine.getCurrentBeliefs(),\n        \"no_reward\",\n      );\n      const result = await performInference(engine, surprise);\n      const afterFE = result.freeEnergy;\n\n      // Should adapt beliefs to reduce surprise\n      expect(result.beliefs.states.exploiting).toBeLessThan(0.9);\n      expect(result.beliefs.uncertainty).toBeGreaterThan(0.1);\n    });\n  });\n});\n"],"names":["describe","engine","testModel","beforeEach","states","observations","actions","transitionModel","exploring","move_forward","exploiting","resting","turn","wait","observationModel","high_reward","low_reward","no_reward","preferences","createActiveInferenceEngine","model","precision","learningRate","planningHorizon","it","expect","toMatchObject","beliefs","getCurrentBeliefs","stateProbs","Object","values","Math","abs","reduce","sum","p","toBeLessThan","forEach","toBeCloseTo","length","invalidModel","toThrow","initialBeliefs","observation","type","value","confidence","updatedBeliefs","updateBeliefs","toBeGreaterThan","highConfObs","lowConfObs","highConfBeliefs","lowConfBeliefs","highConfChange","lowConfChange","obs","s","uncertainty","freeEnergy","calculateFreeEnergy","Number","isFinite","toBe","pessimisticBeliefs","highError","optimisticBeliefs","lowError","certainBeliefs","uncertainBeliefs","certainFE","uncertainFE","policy","action","timestep","efe","calculateExpectedFreeEnergy","total","epistemic","pragmatic","toHaveProperty","exploratoryPolicy","exploitativePolicy","exploratory","exploitative","shortPolicy","longPolicy","shortEFE","longEFE","not","currentBeliefs","selectedAction","selectAction","toContain","toBeLessThanOrEqual","lowUncertainty","highUncertainty","lowUncAction","highUncAction","constrainedEngine","actionConstraints","maxFrequency","i","push","waitFrequency","filter","a","result","performInference","results","startTime","Date","now","iterations","totalTime","avgTime","epistemicValue","calculateEpistemic","toBeGreaterThanOrEqual","pragmaticValue","calculatePragmatic","explore","exploit","initialFE","optimized","minimizeVariationalFreeEnergy","maxIterations","finalFE","tolerance","converged","predictions","predictSensoryOutcomes","totalProb","movePredict","waitPredict","evaluation","evaluateActionPolicy","greedyPolicy","balancedPolicy","greedyEval","balancedEval","session","freeEnergies","t","random","uniqueActions","Set","size","avgEarlyFE","slice","b","avgLateFE","setBeliefs","surprise","beforeFE","afterFE"],"mappings":"AAAA;;;;;CAKC;;;;iCAmBM;AAEPA,SAAS,2BAA2B;IAClC,IAAIC;IACJ,IAAIC;IAEJC,WAAW;QACTD,YAAY;YACVE,QAAQ;gBAAC;gBAAa;gBAAc;aAAU;YAC9CC,cAAc;gBAAC;gBAAe;gBAAc;aAAY;YACxDC,SAAS;gBAAC;gBAAgB;gBAAQ;aAAO;YAEzC,gCAAgC;YAChCC,iBAAiB;gBACfC,WAAW;oBACTC,cAAc;wBAAED,WAAW;wBAAKE,YAAY;wBAAKC,SAAS;oBAAI;oBAC9DC,MAAM;wBAAEJ,WAAW;wBAAKE,YAAY;wBAAKC,SAAS;oBAAI;oBACtDE,MAAM;wBAAEL,WAAW;wBAAKE,YAAY;wBAAKC,SAAS;oBAAI;gBACxD;gBACAD,YAAY;oBACVD,cAAc;wBAAED,WAAW;wBAAKE,YAAY;wBAAKC,SAAS;oBAAI;oBAC9DC,MAAM;wBAAEJ,WAAW;wBAAKE,YAAY;wBAAKC,SAAS;oBAAI;oBACtDE,MAAM;wBAAEL,WAAW;wBAAKE,YAAY;wBAAKC,SAAS;oBAAI;gBACxD;gBACAA,SAAS;oBACPF,cAAc;wBAAED,WAAW;wBAAKE,YAAY;wBAAKC,SAAS;oBAAI;oBAC9DC,MAAM;wBAAEJ,WAAW;wBAAKE,YAAY;wBAAKC,SAAS;oBAAI;oBACtDE,MAAM;wBAAEL,WAAW;wBAAKE,YAAY;wBAAKC,SAAS;oBAAI;gBACxD;YACF;YAEA,2BAA2B;YAC3BG,kBAAkB;gBAChBN,WAAW;oBAAEO,aAAa;oBAAKC,YAAY;oBAAKC,WAAW;gBAAI;gBAC/DP,YAAY;oBAAEK,aAAa;oBAAKC,YAAY;oBAAKC,WAAW;gBAAI;gBAChEN,SAAS;oBAAEI,aAAa;oBAAKC,YAAY;oBAAKC,WAAW;gBAAI;YAC/D;YAEA,iDAAiD;YACjDC,aAAa;gBACXH,aAAa,CAAC;gBACdC,YAAY,CAAC;gBACbC,WAAW;YACb;QACF;QAEAhB,SAASkB,IAAAA,4CAA2B,EAAC;YACnCC,OAAOlB;YACPmB,WAAW;YACXC,cAAc;YACdC,iBAAiB;QACnB;IACF;IAEAvB,SAAS,qCAAqC;QAC5CwB,GAAG,2CAA2C;YAC5CC,OAAOxB,QAAQyB,aAAa,CAAC;gBAC3BN,OAAOlB;gBACPmB,WAAW;gBACXC,cAAc;gBACdC,iBAAiB;YACnB;QACF;QAEAC,GAAG,oCAAoC;YACrC,MAAMG,UAAU1B,OAAO2B,iBAAiB;YACxC,MAAMC,aAAaC,OAAOC,MAAM,CAACJ,QAAQvB,MAAM;YAE/CqB,OACEO,KAAKC,GAAG,CAACJ,WAAWK,MAAM,CAAC,CAACC,KAAKC,IAAMD,MAAMC,GAAG,KAAK,MACrDC,YAAY,CAAC;YACfR,WAAWS,OAAO,CAAC,CAACF;gBAClBX,OAAOW,GAAGG,WAAW,CAAC,MAAMrC,UAAUE,MAAM,CAACoC,MAAM,EAAE;YACvD;QACF;QAEAhB,GAAG,6BAA6B;YAC9B,MAAMiB,eAAe;gBACnB,GAAGvC,SAAS;gBACZK,iBAAiB,CAAC;YACpB;YAEAkB,OAAO;gBACLN,IAAAA,4CAA2B,EAAC;oBAAEC,OAAOqB;gBAAa;YACpD,GAAGC,OAAO,CAAC;QACb;IACF;IAEA1C,SAAS,kBAAkB;QACzBwB,GAAG,yCAAyC;YAC1C,MAAMmB,iBAAiB1C,OAAO2B,iBAAiB;YAC/C,MAAMgB,cAA4B;gBAChCC,MAAM;gBACNC,OAAO;gBACPC,YAAY;YACd;YAEA,MAAMC,iBAAiBC,IAAAA,8BAAa,EAAChD,QAAQ2C;YAE7C,uEAAuE;YACvEnB,OAAOuB,eAAe5C,MAAM,CAACM,UAAU,EAAEwC,eAAe,CACtDP,eAAevC,MAAM,CAACM,UAAU;QAEpC;QAEAc,GAAG,uCAAuC;YACxC,MAAM2B,cAA4B;gBAChCN,MAAM;gBACNC,OAAO;gBACPC,YAAY;YACd;YAEA,MAAMK,aAA2B;gBAC/BP,MAAM;gBACNC,OAAO;gBACPC,YAAY;YACd;YAEA,MAAMM,kBAAkBJ,IAAAA,8BAAa,EAAChD,QAAQkD;YAC9C,MAAMG,iBAAiBL,IAAAA,8BAAa,EAAChD,QAAQmD;YAE7C,yDAAyD;YACzD,MAAMG,iBAAiBvB,KAAKC,GAAG,CAC7BoB,gBAAgBjD,MAAM,CAACM,UAAU,GAAG,IAAI;YAE1C,MAAM8C,gBAAgBxB,KAAKC,GAAG,CAACqB,eAAelD,MAAM,CAACM,UAAU,GAAG,IAAI;YAEtEe,OAAO8B,gBAAgBL,eAAe,CAACM;QACzC;QAEAhC,GAAG,uCAAuC;YACxC,MAAMnB,eAAe;gBACnB;oBAAEwC,MAAM;oBAAeC,OAAO;oBAAeC,YAAY;gBAAI;gBAC7D;oBAAEF,MAAM;oBAAeC,OAAO;oBAAcC,YAAY;gBAAI;gBAC5D;oBAAEF,MAAM;oBAAeC,OAAO;oBAAaC,YAAY;gBAAI;aAC5D;YAED1C,aAAaiC,OAAO,CAAC,CAACmB;gBACpB,MAAM9B,UAAUsB,IAAAA,8BAAa,EAAChD,QAAQwD;gBACtC,MAAMtB,MAAML,OAAOC,MAAM,CAACJ,QAAQvB,MAAM,EAAE8B,MAAM,CAAC,CAACwB,GAAGtB,IAAMsB,IAAItB,GAAG;gBAClEX,OAAOU,KAAKI,WAAW,CAAC,KAAK;YAC/B;QACF;IACF;IAEAvC,SAAS,4BAA4B;QACnCwB,GAAG,sCAAsC;YACvC,MAAMG,UAAmB;gBACvBvB,QAAQ;oBAAEI,WAAW;oBAAKE,YAAY;oBAAKC,SAAS;gBAAI;gBACxDgD,aAAa;YACf;YAEA,MAAMf,cAAc;YACpB,MAAMgB,aAAa3D,OAAO4D,mBAAmB,CAAClC,SAASiB;YAEvDnB,OAAOmC,YAAYV,eAAe,CAAC;YACnCzB,OAAOqC,OAAOC,QAAQ,CAACH,aAAaI,IAAI,CAAC;QAC3C;QAEAxC,GAAG,mCAAmC;YACpC,yCAAyC;YACzC,MAAMyC,qBAA8B;gBAClC7D,QAAQ;oBAAEI,WAAW;oBAAKE,YAAY;oBAAKC,SAAS;gBAAI;gBACxDgD,aAAa;YACf;YAEA,2BAA2B;YAC3B,MAAMf,cAAc;YAEpB,MAAMsB,YAAYjE,OAAO4D,mBAAmB,CAC1CI,oBACArB;YAGF,0CAA0C;YAC1C,MAAMuB,oBAA6B;gBACjC/D,QAAQ;oBAAEI,WAAW;oBAAKE,YAAY;oBAAKC,SAAS;gBAAI;gBACxDgD,aAAa;YACf;YAEA,MAAMS,WAAWnE,OAAO4D,mBAAmB,CACzCM,mBACAvB;YAGFnB,OAAOyC,WAAWhB,eAAe,CAACkB;QACpC;QAEA5C,GAAG,qCAAqC;YACtC,MAAM6C,iBAA0B;gBAC9BjE,QAAQ;oBAAEI,WAAW;oBAAME,YAAY;oBAAMC,SAAS;gBAAK;gBAC3DgD,aAAa;YACf;YAEA,MAAMW,mBAA4B;gBAChClE,QAAQ;oBAAEI,WAAW;oBAAME,YAAY;oBAAMC,SAAS;gBAAK;gBAC3DgD,aAAa;YACf;YAEA,MAAMf,cAAc;YACpB,MAAM2B,YAAYtE,OAAO4D,mBAAmB,CAACQ,gBAAgBzB;YAC7D,MAAM4B,cAAcvE,OAAO4D,mBAAmB,CAC5CS,kBACA1B;YAGF,sDAAsD;YACtDnB,OAAOO,KAAKC,GAAG,CAACsC,YAAYC,cAActB,eAAe,CAAC;QAC5D;IACF;IAEAlD,SAAS,wBAAwB;QAC/BwB,GAAG,gDAAgD;YACjD,MAAMiD,SAAiB;gBACrB;oBAAEC,QAAQ;oBAAgBC,UAAU;gBAAE;gBACtC;oBAAED,QAAQ;oBAAQC,UAAU;gBAAE;gBAC9B;oBAAED,QAAQ;oBAAQC,UAAU;gBAAE;aAC/B;YAED,MAAMC,MAAMC,IAAAA,4CAA2B,EAAC5E,QAAQwE;YAEhDhD,OAAOqC,OAAOC,QAAQ,CAACa,IAAIE,KAAK,GAAGd,IAAI,CAAC;YACxCvC,OAAOqC,OAAOC,QAAQ,CAACa,IAAIG,SAAS,GAAGf,IAAI,CAAC;YAC5CvC,OAAOqC,OAAOC,QAAQ,CAACa,IAAII,SAAS,GAAGhB,IAAI,CAAC;YAC5CvC,OAAOmD,KAAKK,cAAc,CAAC;YAC3BxD,OAAOmD,KAAKK,cAAc,CAAC;YAC3BxD,OAAOmD,KAAKK,cAAc,CAAC;QAC7B;QAEAzD,GAAG,0CAA0C;YAC3C,MAAM0D,oBAA4B;gBAChC;oBAAER,QAAQ;oBAAQC,UAAU;gBAAE;gBAC9B;oBAAED,QAAQ;oBAAgBC,UAAU;gBAAE;aACvC;YAED,MAAMQ,qBAA6B;gBACjC;oBAAET,QAAQ;oBAAQC,UAAU;gBAAE;gBAC9B;oBAAED,QAAQ;oBAAQC,UAAU;gBAAE;aAC/B;YAED,MAAMS,cAAcP,IAAAA,4CAA2B,EAC7C5E,QACAiF;YAEF,MAAMG,eAAeR,IAAAA,4CAA2B,EAC9C5E,QACAkF;YAGF1D,OAAO2D,YAAYL,SAAS,EAAE1C,YAAY,CAACgD,aAAaN,SAAS;QACnE;QAEAvD,GAAG,6BAA6B;YAC9B,MAAM8D,cAAsB;gBAAC;oBAAEZ,QAAQ;oBAAgBC,UAAU;gBAAE;aAAE;YAErE,MAAMY,aAAqB;gBACzB;oBAAEb,QAAQ;oBAAgBC,UAAU;gBAAE;gBACtC;oBAAED,QAAQ;oBAAgBC,UAAU;gBAAE;gBACtC;oBAAED,QAAQ;oBAAgBC,UAAU;gBAAE;aACvC;YAED,MAAMa,WAAWX,IAAAA,4CAA2B,EAAC5E,QAAQqF;YACrD,MAAMG,UAAUZ,IAAAA,4CAA2B,EAAC5E,QAAQsF;YAEpD,0DAA0D;YAC1D9D,OAAO+D,SAASV,KAAK,EAAEY,GAAG,CAAC1B,IAAI,CAACyB,QAAQX,KAAK;QAC/C;IACF;IAEA9E,SAAS,oBAAoB;QAC3BwB,GAAG,sDAAsD;YACvD,MAAMmE,iBAA0B;gBAC9BvF,QAAQ;oBAAEI,WAAW;oBAAKE,YAAY;oBAAKC,SAAS;gBAAI;gBACxDgD,aAAa;YACf;YAEA,MAAMiC,iBAAiBC,IAAAA,6BAAY,EAAC5F,QAAQ0F;YAE5ClE,OAAOvB,UAAUI,OAAO,EAAEwF,SAAS,CAACF,eAAe/C,IAAI;YACvDpB,OAAOmE,eAAe7C,UAAU,EAAEG,eAAe,CAAC;YAClDzB,OAAOmE,eAAe7C,UAAU,EAAEgD,mBAAmB,CAAC;QACxD;QAEAvE,GAAG,gDAAgD;YACjD,MAAMwE,iBAA0B;gBAC9B5F,QAAQ;oBAAEI,WAAW;oBAAKE,YAAY;oBAAKC,SAAS;gBAAI;gBACxDgD,aAAa;YACf;YAEA,MAAMsC,kBAA2B;gBAC/B7F,QAAQ;oBAAEI,WAAW;oBAAKE,YAAY;oBAAKC,SAAS;gBAAI;gBACxDgD,aAAa;YACf;YAEA,MAAMuC,eAAeL,IAAAA,6BAAY,EAAC5F,QAAQ+F;YAC1C,MAAMG,gBAAgBN,IAAAA,6BAAY,EAAC5F,QAAQgG;YAE3C,oDAAoD;YACpDxE,OAAO0E,cAActD,IAAI,EAAE6C,GAAG,CAAC1B,IAAI,CAAC;QACtC;QAEAxC,GAAG,+BAA+B;YAChC,MAAM4E,oBAAoBjF,IAAAA,4CAA2B,EAAC;gBACpDC,OAAOlB;gBACPmG,mBAAmB;oBACjBxF,MAAM;wBAAEyF,cAAc;oBAAI;gBAC5B;YACF;YAEA,MAAMhG,UAAU,EAAE;YAClB,IAAK,IAAIiG,IAAI,GAAGA,IAAI,KAAKA,IAAK;gBAC5B,MAAM7B,SAASmB,IAAAA,6BAAY,EACzBO,mBACAnG,OAAO2B,iBAAiB;gBAE1BtB,QAAQkG,IAAI,CAAC9B,OAAO7B,IAAI;YAC1B;YAEA,MAAM4D,gBACJnG,QAAQoG,MAAM,CAAC,CAACC,IAAMA,MAAM,QAAQnE,MAAM,GAAGlC,QAAQkC,MAAM;YAC7Df,OAAOgF,eAAeV,mBAAmB,CAAC,OAAO,uBAAuB;QAC1E;IACF;IAEA/F,SAAS,qBAAqB;QAC5BwB,GAAG,qCAAqC;YACtC,MAAMoB,cAA4B;gBAChCC,MAAM;gBACNC,OAAO;gBACPC,YAAY;YACd;YAEA,MAAM6D,SAAS,MAAMC,IAAAA,iCAAgB,EAAC5G,QAAQ2C;YAE9CnB,OAAOmF,QAAQ3B,cAAc,CAAC;YAC9BxD,OAAOmF,QAAQ3B,cAAc,CAAC;YAC9BxD,OAAOmF,QAAQ3B,cAAc,CAAC;YAC9BxD,OAAOmF,QAAQ3B,cAAc,CAAC;QAChC;QAEAzD,GAAG,mCAAmC;YACpC,MAAMnB,eAAe;gBACnB;oBAAEwC,MAAM;oBAAeC,OAAO;oBAAaC,YAAY;gBAAI;gBAC3D;oBAAEF,MAAM;oBAAeC,OAAO;oBAAcC,YAAY;gBAAI;gBAC5D;oBAAEF,MAAM;oBAAeC,OAAO;oBAAeC,YAAY;gBAAK;aAC/D;YAED,MAAM+D,UAAU,EAAE;YAClB,KAAK,MAAMrD,OAAOpD,aAAc;gBAC9B,MAAMuG,SAAS,MAAMC,IAAAA,iCAAgB,EAAC5G,QAAQwD;gBAC9CqD,QAAQN,IAAI,CAACI;YACf;YAEA,qDAAqD;YACrDnF,OAAOqF,OAAO,CAAC,EAAE,CAAClB,cAAc,CAAC/C,IAAI,EAAE6C,GAAG,CAAC1B,IAAI,CAC7C8C,OAAO,CAAC,EAAE,CAAClB,cAAc,CAAC/C,IAAI;QAElC;QAEArB,GAAG,sCAAsC;YACvC,MAAMuF,YAAYC,KAAKC,GAAG;YAC1B,MAAMC,aAAa;YAEnB,IAAK,IAAIX,IAAI,GAAGA,IAAIW,YAAYX,IAAK;gBACnC,MAAM9C,MAAoB;oBACxBZ,MAAM;oBACNC,OAAO;wBAAC;wBAAe;wBAAc;qBAAY,CAACyD,IAAI,EAAE;oBACxDxD,YAAY;gBACd;gBACA,MAAM8D,IAAAA,iCAAgB,EAAC5G,QAAQwD;YACjC;YAEA,MAAM0D,YAAYH,KAAKC,GAAG,KAAKF;YAC/B,MAAMK,UAAUD,YAAYD;YAE5BzF,OAAO2F,SAAS/E,YAAY,CAAC,KAAK,wCAAwC;QAC5E;IACF;IAEArC,SAAS,iCAAiC;QACxCwB,GAAG,iDAAiD;YAClD,MAAMG,UAAmB;gBACvBvB,QAAQ;oBAAEI,WAAW;oBAAKE,YAAY;oBAAKC,SAAS;gBAAI;gBACxDgD,aAAa;YACf;YAEA,MAAMe,SAAS;YACf,MAAM2C,iBAAiBC,IAAAA,mCAAkB,EAACrH,QAAQ0B,SAAS+C;YAE3DjD,OAAO4F,gBAAgBE,sBAAsB,CAAC;YAC9C9F,OAAOqC,OAAOC,QAAQ,CAACsD,iBAAiBrD,IAAI,CAAC;QAC/C;QAEAxC,GAAG,iDAAiD;YAClD,MAAMG,UAAmB;gBACvBvB,QAAQ;oBAAEI,WAAW;oBAAKE,YAAY;oBAAKC,SAAS;gBAAI;gBACxDgD,aAAa;YACf;YAEA,MAAMe,SAAS;YACf,MAAM8C,iBAAiBC,IAAAA,mCAAkB,EAACxH,QAAQ0B,SAAS+C;YAE3DjD,OAAOqC,OAAOC,QAAQ,CAACyD,iBAAiBxD,IAAI,CAAC;QAC7C,wDAAwD;QAC1D;QAEAxC,GAAG,0CAA0C;YAC3C,MAAMG,UAAmB;gBACvBvB,QAAQ;oBAAEI,WAAW;oBAAKE,YAAY;oBAAKC,SAAS;gBAAI;gBACxDgD,aAAa;YACf;YAEA,MAAM+D,UAAUJ,IAAAA,mCAAkB,EAACrH,QAAQ0B,SAAS;YACpD,MAAMgG,UAAUF,IAAAA,mCAAkB,EAACxH,QAAQ0B,SAAS;YAEpD,wCAAwC;YACxCF,OAAOiG,SAASxE,eAAe,CAAC;YAChCzB,OAAOO,KAAKC,GAAG,CAAC0F,UAAUzE,eAAe,CAAC;QAC5C;IACF;IAEAlD,SAAS,wCAAwC;QAC/CwB,GAAG,kDAAkD;YACnD,MAAMmB,iBAA0B;gBAC9BvC,QAAQ;oBAAEI,WAAW;oBAAKE,YAAY;oBAAKC,SAAS;gBAAI;gBACxDgD,aAAa;YACf;YAEA,MAAMf,cAAc;YACpB,MAAMgF,YAAY3H,OAAO4D,mBAAmB,CAAClB,gBAAgBC;YAE7D,MAAMiF,YAAY,MAAMC,IAAAA,8CAA6B,EACnD7H,QACA0C,gBACAC,aACA;gBAAEmF,eAAe;YAAG;YAGtB,MAAMC,UAAU/H,OAAO4D,mBAAmB,CAACgE,WAAWjF;YAEtDnB,OAAOuG,SAAS3F,YAAY,CAACuF;QAC/B;QAEApG,GAAG,gCAAgC;YACjC,MAAMG,UAAmB;gBACvBvB,QAAQ;oBAAEI,WAAW;oBAAME,YAAY;oBAAMC,SAAS;gBAAK;gBAC3DgD,aAAa;YACf;YAEA,MAAMf,cAAc;YACpB,MAAMiF,YAAY,MAAMC,IAAAA,8CAA6B,EACnD7H,QACA0B,SACAiB,aACA;gBAAEmF,eAAe;gBAAKE,WAAW;YAAM;YAGzC,oCAAoC;YACpCxG,OAAOoG,UAAUK,SAAS,EAAElE,IAAI,CAAC;YACjCvC,OAAOoG,UAAUX,UAAU,EAAE7E,YAAY,CAAC;QAC5C;IACF;IAEArC,SAAS,yBAAyB;QAChCwB,GAAG,oCAAoC;YACrC,MAAMmE,iBAA0B;gBAC9BvF,QAAQ;oBAAEI,WAAW;oBAAKE,YAAY;oBAAKC,SAAS;gBAAI;gBACxDgD,aAAa;YACf;YAEA,MAAMe,SAAS;YACf,MAAMyD,cAAcC,IAAAA,uCAAsB,EACxCnI,QACA0F,gBACAjB;YAGFjD,OAAO0G,aAAalD,cAAc,CAAC;YACnCxD,OAAO0G,aAAalD,cAAc,CAAC;YACnCxD,OAAO0G,aAAalD,cAAc,CAAC;YAEnC,MAAMoD,YAAYvG,OAAOC,MAAM,CAACoG,aAAajG,MAAM,CACjD,CAACC,KAAKC,IAAMD,MAAMC,GAClB;YAEFX,OAAO4G,WAAW9F,WAAW,CAAC,KAAK;QACrC;QAEAf,GAAG,wCAAwC;YACzC,MAAMG,UAAmB;gBACvBvB,QAAQ;oBAAEI,WAAW;oBAAKE,YAAY;oBAAKC,SAAS;gBAAI;gBACxDgD,aAAa;YACf;YAEA,MAAM2E,cAAcF,IAAAA,uCAAsB,EACxCnI,QACA0B,SACA;YAEF,MAAM4G,cAAcH,IAAAA,uCAAsB,EAACnI,QAAQ0B,SAAS;YAE5D,yDAAyD;YACzDF,OAAO6G,YAAYvH,WAAW,EAAE2E,GAAG,CAAC1B,IAAI,CAACuE,YAAYxH,WAAW;QAClE;IACF;IAEAf,SAAS,qBAAqB;QAC5BwB,GAAG,8BAA8B;YAC/B,MAAMiD,SAAiB;gBACrB;oBAAEC,QAAQ;oBAAgBC,UAAU;gBAAE;gBACtC;oBAAED,QAAQ;oBAAgBC,UAAU;gBAAE;gBACtC;oBAAED,QAAQ;oBAAQC,UAAU;gBAAE;aAC/B;YAED,MAAM6D,aAAaC,IAAAA,qCAAoB,EAACxI,QAAQwE;YAEhDhD,OAAO+G,YAAYvD,cAAc,CAAC;YAClCxD,OAAO+G,YAAYvD,cAAc,CAAC;YAClCxD,OAAO+G,YAAYvD,cAAc,CAAC;QACpC;QAEAzD,GAAG,iCAAiC;YAClC,MAAMkH,eAAuB;gBAC3B;oBAAEhE,QAAQ;oBAAQC,UAAU;gBAAE;gBAC9B;oBAAED,QAAQ;oBAAQC,UAAU;gBAAE;aAC/B;YAED,MAAMgE,iBAAyB;gBAC7B;oBAAEjE,QAAQ;oBAAgBC,UAAU;gBAAE;gBACtC;oBAAED,QAAQ;oBAAQC,UAAU;gBAAE;aAC/B;YAED,MAAMiE,aAAaH,IAAAA,qCAAoB,EAACxI,QAAQyI;YAChD,MAAMG,eAAeJ,IAAAA,qCAAoB,EAACxI,QAAQ0I;YAElD,iDAAiD;YACjDlH,OAAOmH,WAAWjF,WAAW,EAAE+B,GAAG,CAAC1B,IAAI,CAAC6E,aAAalF,WAAW;QAClE;IACF;IAEA3D,SAAS,qBAAqB;QAC5BwB,GAAG,gCAAgC;YACjC,MAAMsH,UAAU;gBACdzI,cAAc;gBACdC,SAAS,EAAE;gBACXqB,SAAS,EAAE;gBACXoH,cAAc,EAAE;YAClB;YAEA,gCAAgC;YAChC,IAAK,IAAIC,IAAI,GAAGA,IAAI,IAAIA,IAAK;gBAC3B,MAAMvF,MAAoB;oBACxBZ,MAAM;oBACNC,OAAOd,KAAKiH,MAAM,KAAK,MAAM,gBAAgB;oBAC7ClG,YAAY,MAAMf,KAAKiH,MAAM,KAAK;gBACpC;gBAEA,MAAMrC,SAAS,MAAMC,IAAAA,iCAAgB,EAAC5G,QAAQwD;gBAE9CqF,QAAQzI,YAAY;gBACpByI,QAAQxI,OAAO,CAACkG,IAAI,CAACI,OAAOhB,cAAc,CAAC/C,IAAI;gBAC/CiG,QAAQnH,OAAO,CAAC6E,IAAI,CAACI,OAAOjF,OAAO;gBACnCmH,QAAQC,YAAY,CAACvC,IAAI,CAACI,OAAOhD,UAAU;YAC7C;YAEA,gCAAgC;YAChC,MAAMsF,gBAAgB,IAAIC,IAAIL,QAAQxI,OAAO,EAAE8I,IAAI;YACnD3H,OAAOyH,eAAehG,eAAe,CAAC;YAEtC,wCAAwC;YACxC,MAAMmG,aACJP,QAAQC,YAAY,CAACO,KAAK,CAAC,GAAG,GAAGpH,MAAM,CAAC,CAACyE,GAAG4C,IAAM5C,IAAI4C,KAAK;YAC7D,MAAMC,YACJV,QAAQC,YAAY,CAACO,KAAK,CAAC,CAAC,GAAGpH,MAAM,CAAC,CAACyE,GAAG4C,IAAM5C,IAAI4C,KAAK;YAC3D9H,OAAO+H,WAAWzD,mBAAmB,CAACsD;QACxC;QAEA7H,GAAG,yCAAyC;YAC1C,qBAAqB;YACrBvB,OAAOwJ,UAAU,CAAC;gBAChBrJ,QAAQ;oBAAEI,WAAW;oBAAME,YAAY;oBAAKC,SAAS;gBAAK;gBAC1DgD,aAAa;YACf;YAEA,yBAAyB;YACzB,MAAM+F,WAAyB;gBAC7B7G,MAAM;gBACNC,OAAO;gBACPC,YAAY;YACd;YAEA,MAAM4G,WAAW1J,OAAO4D,mBAAmB,CACzC5D,OAAO2B,iBAAiB,IACxB;YAEF,MAAMgF,SAAS,MAAMC,IAAAA,iCAAgB,EAAC5G,QAAQyJ;YAC9C,MAAME,UAAUhD,OAAOhD,UAAU;YAEjC,0CAA0C;YAC1CnC,OAAOmF,OAAOjF,OAAO,CAACvB,MAAM,CAACM,UAAU,EAAE2B,YAAY,CAAC;YACtDZ,OAAOmF,OAAOjF,OAAO,CAACgC,WAAW,EAAET,eAAe,CAAC;QACrD;IACF;AACF"}