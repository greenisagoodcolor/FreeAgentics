"""Selective Update Optimization Benchmarks for PyMDP.

Measures the impact of selective updates on belief states, partial policy updates,
and incremental free energy calculations. Tests scenarios with sparse observations,
partial state changes, and hierarchical model updates to quantify optimization benefits.
"""

import json
import time
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional, Set, Tuple

import numpy as np
from pymdp_benchmarks import BenchmarkResult, BenchmarkSuite, PyMDPBenchmark

# Try to import PyMDP
try:
    import pymdp
    from pymdp import utils
    from pymdp.agent import Agent as PyMDPAgent
    from pymdp.maths import softmax, spm_dot
    from pymdp.maths import spm_log_single as log_stable

    PYMDP_AVAILABLE = True
except ImportError:
    PYMDP_AVAILABLE = False


@dataclass
class SelectiveUpdateMetrics:
    """Container for selective update performance metrics."""

    full_update_time_ms: float
    selective_update_time_ms: float
    speedup_factor: float
    computation_savings_percent: float
    accuracy_maintained_percent: float
    operations_skipped: int
    operations_total: int


class SparseObservationBenchmark(PyMDPBenchmark):
    """Benchmark selective updates with sparse observations."""

    def __init__(
        self,
        state_size: int = 30,
        sparsity_level: float = 0.1,
        selective_enabled: bool = True,
    ):
        """Initialize sparse observation benchmark with configuration parameters.

        Args:
            state_size: Size of the state space
            sparsity_level: Fraction of observations that change
            selective_enabled: Whether to use selective updates
        """
        super().__init__(
            f"sparse_observation_{'selective' if selective_enabled else 'full'}"
        )
        self.state_size = state_size
        self.sparsity_level = (
            sparsity_level  # Fraction of observations that change
        )
        self.selective_enabled = selective_enabled
        self.agent = None
        self.A_matrices = None
        self.previous_observations = None
        self.change_tracker = ChangeTracker()

    def setup(self):
        """Initialize agent and observation structures."""
        if not PYMDP_AVAILABLE:
            return

        # Create agent with multiple modalities
        num_states = [self.state_size] * 3
        num_obs = [self.state_size] * 3
        num_actions = 4

        self.A_matrices = utils.random_A_matrix(num_obs, num_states)
        B_matrices = utils.random_B_matrix(num_states, num_actions)
        C_matrices = utils.obj_array_uniform(num_obs)

        self.agent = PyMDPAgent(self.A_matrices, B_matrices, C=C_matrices)

        # Initialize with baseline observations
        self.previous_observations = [0] * len(num_obs)
        self.change_tracker.reset()

    def run_iteration(self) -> Dict[str, Any]:
        """Run belief update with sparse observations."""
        if not PYMDP_AVAILABLE:
            raise ImportError("PyMDP is required for this benchmark")

        # Generate new observations with controlled sparsity
        new_observations = self._generate_sparse_observations()
        changed_modalities = self._detect_changes(new_observations)

        if self.selective_enabled:
            # Selective update - only update changed modalities
            start_time = time.perf_counter()
            self._selective_belief_update(new_observations, changed_modalities)
            selective_time = (time.perf_counter() - start_time) * 1000

            # For comparison, also time full update
            start_time = time.perf_counter()
            self._full_belief_update(new_observations)
            full_time = (time.perf_counter() - start_time) * 1000

            computation_savings = (
                1 - len(changed_modalities) / len(new_observations)
            ) * 100
            speedup = full_time / selective_time if selective_time > 0 else 1.0

        else:
            # Full update only
            start_time = time.perf_counter()
            self._full_belief_update(new_observations)
            full_time = (time.perf_counter() - start_time) * 1000

            selective_time = full_time
            computation_savings = 0.0
            speedup = 1.0

        self.previous_observations = new_observations

        return {
            "full_update_time_ms": full_time,
            "selective_update_time_ms": selective_time,
            "speedup_factor": speedup,
            "computation_savings_percent": computation_savings,
            "changed_modalities": len(changed_modalities),
            "total_modalities": len(new_observations),
            "sparsity_achieved": len(changed_modalities)
            / len(new_observations),
            "operations_skipped": len(new_observations)
            - len(changed_modalities),
        }

    def _generate_sparse_observations(self) -> List[int]:
        """Generate observations with controlled sparsity."""
        new_obs = self.previous_observations.copy()

        # Determine which modalities to change
        num_to_change = max(1, int(len(new_obs) * self.sparsity_level))
        modalities_to_change = np.random.choice(
            len(new_obs), num_to_change, replace=False
        )

        # Change selected modalities
        for mod_idx in modalities_to_change:
            new_obs[mod_idx] = np.random.randint(0, self.state_size)

        return new_obs

    def _detect_changes(self, new_observations: List[int]) -> Set[int]:
        """Detect which modalities changed from previous observations."""
        changed = set()
        for i, (prev, new) in enumerate(
            zip(self.previous_observations, new_observations)
        ):
            if prev != new:
                changed.add(i)
        return changed

    def _selective_belief_update(
        self, observations: List[int], changed_modalities: Set[int]
    ):
        """Perform selective belief update only for changed modalities."""
        # Simplified selective update - in practice would integrate with PyMDP's internals
        if changed_modalities:
            # Update beliefs only for changed modalities
            for mod_idx in changed_modalities:
                self._update_modality_belief(mod_idx, observations[mod_idx])
        else:
            # No changes, no update needed
            pass

    def _full_belief_update(self, observations: List[int]):
        """Perform full belief update for all modalities."""
        self.agent.infer_states(observations)

    def _update_modality_belief(self, modality_idx: int, observation: int):
        """Update belief for a specific modality (simplified)."""
        # This would integrate with PyMDP's belief update mechanism
        # For benchmarking, we simulate the computational work
        A_matrix = self.A_matrices[modality_idx]
        likelihood = A_matrix[observation]

        # Simulate belief computation work
        for _ in range(5):  # Simulated computational steps
            normalized = softmax(likelihood)
            likelihood = np.log(normalized + 1e-16)

    def get_configuration(self) -> Dict[str, Any]:
        return {
            "state_size": self.state_size,
            "sparsity_level": self.sparsity_level,
            "selective_enabled": self.selective_enabled,
            "num_modalities": 3,
        }


class PartialPolicyUpdateBenchmark(PyMDPBenchmark):
    """Benchmark selective policy updates for unchanged state factors."""

    def __init__(
        self,
        state_size: int = 20,
        num_policies: int = 50,
        update_fraction: float = 0.3,
    ):
        """Initialize partial policy update benchmark.

        Args:
            state_size: Size of the state space
            num_policies: Number of policies to manage
            update_fraction: Fraction of policies that need updating
        """
        super().__init__(
            f"partial_policy_update_{int(update_fraction*100)}pct"
        )
        self.state_size = state_size
        self.num_policies = num_policies
        self.update_fraction = (
            update_fraction  # Fraction of policies that need updating
        )
        self.agent = None
        self.policy_cache = {}
        self.state_hash_tracker = StateHashTracker()

    def setup(self):
        """Initialize agent with policy structure."""
        if not PYMDP_AVAILABLE:
            return

        num_states = [self.state_size] * 2
        num_obs = [self.state_size] * 2
        num_actions = 4

        A_matrices = utils.random_A_matrix(num_obs, num_states)
        B_matrices = utils.random_B_matrix(num_states, num_actions)
        C_matrices = utils.obj_array_uniform(num_obs)

        self.agent = PyMDPAgent(
            A_matrices,
            B_matrices,
            C=C_matrices,
            planning_horizon=3,
            inference_horizon=1,
        )

        self.policy_cache.clear()
        self.state_hash_tracker.reset()

    def run_iteration(self) -> Dict[str, Any]:
        """Run policy update with selective optimization."""
        if not PYMDP_AVAILABLE:
            raise ImportError("PyMDP is required for this benchmark")

        # Generate observations and update beliefs
        observations = [
            np.random.randint(0, self.state_size) for _ in range(2)
        ]
        self.agent.infer_states(observations)

        # Track state changes
        current_state_hash = self.state_hash_tracker.compute_state_hash(
            self.agent.qs
        )
        changed_factors = self.state_hash_tracker.get_changed_factors(
            current_state_hash
        )

        # Determine which policies need updating based on changed state factors
        policies_to_update = self._determine_policies_to_update(
            changed_factors
        )

        # Perform selective policy update
        start_time = time.perf_counter()
        self._selective_policy_update(policies_to_update)
        selective_time = (time.perf_counter() - start_time) * 1000

        # For comparison, perform full policy update
        start_time = time.perf_counter()
        self._full_policy_update()
        full_time = (time.perf_counter() - start_time) * 1000

        # Calculate metrics
        policies_updated = len(policies_to_update)
        total_policies = self.num_policies
        computation_savings = (1 - policies_updated / total_policies) * 100
        speedup = full_time / selective_time if selective_time > 0 else 1.0

        return {
            "full_update_time_ms": full_time,
            "selective_update_time_ms": selective_time,
            "speedup_factor": speedup,
            "computation_savings_percent": computation_savings,
            "policies_updated": policies_updated,
            "total_policies": total_policies,
            "update_efficiency": policies_updated / total_policies,
            "changed_factors": len(changed_factors),
        }

    def _determine_policies_to_update(
        self, changed_factors: Set[int]
    ) -> List[int]:
        """Determine which policies need updating based on changed state factors."""
        # Simulate policy dependency on state factors
        policies_to_update = []

        num_policies_affected = max(
            1, int(self.num_policies * self.update_fraction)
        )

        # In practice, this would analyze policy dependencies on state factors
        # For benchmarking, we simulate a realistic subset
        policies_to_update = list(range(num_policies_affected))

        return policies_to_update

    def _selective_policy_update(self, policies_to_update: List[int]):
        """Update only the specified policies."""
        # Simulate selective policy computation
        for policy_idx in policies_to_update:
            self._compute_policy_value(policy_idx)

    def _full_policy_update(self):
        """Perform full policy update."""
        self.agent.infer_policies()

    def _compute_policy_value(self, policy_idx: int):
        """Compute value for a specific policy (simulated)."""
        # Simulate policy evaluation computation
        for _ in range(10):  # Simulated computation steps
            # This would be the actual policy evaluation in PyMDP
            dummy_computation = np.random.rand(
                self.state_size, self.state_size
            )
            dummy_computation = softmax(dummy_computation.flatten()).reshape(
                self.state_size, self.state_size
            )

    def get_configuration(self) -> Dict[str, Any]:
        return {
            "state_size": self.state_size,
            "num_policies": self.num_policies,
            "update_fraction": self.update_fraction,
            "planning_horizon": 3,
        }


class IncrementalFreeEnergyBenchmark(PyMDPBenchmark):
    """Benchmark incremental free energy calculations."""

    def __init__(self, state_size: int = 25, incremental_enabled: bool = True):
        """Initialize incremental free energy benchmark.

        Args:
            state_size: Size of the state space
            incremental_enabled: Whether to use incremental calculations
        """
        super().__init__(
            f"incremental_free_energy_{'enabled' if incremental_enabled else 'disabled'}"
        )
        self.state_size = state_size
        self.incremental_enabled = incremental_enabled
        self.agent = None
        self.fe_cache = FreeEnergyCache()
        self.previous_beliefs = None

    def setup(self):
        """Initialize agent for free energy calculations."""
        if not PYMDP_AVAILABLE:
            return

        num_states = [self.state_size] * 2
        num_obs = [self.state_size] * 2
        num_actions = 4

        A_matrices = utils.random_A_matrix(num_obs, num_states)
        B_matrices = utils.random_B_matrix(num_states, num_actions)
        C_matrices = utils.obj_array_uniform(num_obs)

        self.agent = PyMDPAgent(A_matrices, B_matrices, C=C_matrices)
        self.fe_cache.reset()
        self.previous_beliefs = None

    def run_iteration(self) -> Dict[str, Any]:
        """Run free energy calculation with incremental optimization."""
        if not PYMDP_AVAILABLE:
            raise ImportError("PyMDP is required for this benchmark")

        # Generate observations and update beliefs
        observations = [
            np.random.randint(0, self.state_size) for _ in range(2)
        ]
        self.agent.infer_states(observations)

        current_beliefs = [q.copy() for q in self.agent.qs]

        if self.incremental_enabled and self.previous_beliefs is not None:
            # Incremental free energy calculation
            start_time = time.perf_counter()
            incremental_fe = self._incremental_free_energy(
                current_beliefs, observations
            )
            incremental_time = (time.perf_counter() - start_time) * 1000

            # For comparison, full calculation
            start_time = time.perf_counter()
            full_fe = self._full_free_energy(current_beliefs, observations)
            full_time = (time.perf_counter() - start_time) * 1000

            # Calculate accuracy difference
            accuracy_maintained = max(
                0, 100 - abs(incremental_fe - full_fe) / abs(full_fe) * 100
            )
            speedup = (
                full_time / incremental_time if incremental_time > 0 else 1.0
            )

        else:
            # Full calculation only
            start_time = time.perf_counter()
            full_fe = self._full_free_energy(current_beliefs, observations)
            full_time = (time.perf_counter() - start_time) * 1000

            incremental_time = full_time
            incremental_fe = full_fe
            accuracy_maintained = 100.0
            speedup = 1.0

        self.previous_beliefs = current_beliefs

        return {
            "full_calculation_time_ms": full_time,
            "incremental_calculation_time_ms": incremental_time,
            "speedup_factor": speedup,
            "accuracy_maintained_percent": accuracy_maintained,
            "full_free_energy": full_fe,
            "incremental_free_energy": incremental_fe,
            "cache_entries": self.fe_cache.size(),
        }

    def _incremental_free_energy(
        self, beliefs: List[np.ndarray], observations: List[int]
    ) -> float:
        """Calculate free energy incrementally using cached components."""
        # Identify which belief factors changed significantly
        changed_factors = self._detect_belief_changes(
            beliefs, self.previous_beliefs
        )

        # Use cached components for unchanged factors
        fe_components = []
        for factor_idx in range(len(beliefs)):
            if factor_idx in changed_factors:
                # Recalculate for changed factor
                component = self._calculate_fe_component(
                    factor_idx, beliefs, observations
                )
                self.fe_cache.update(factor_idx, component)
            else:
                # Use cached component
                component = self.fe_cache.get(factor_idx)
                if component is None:
                    component = self._calculate_fe_component(
                        factor_idx, beliefs, observations
                    )
                    self.fe_cache.update(factor_idx, component)

            fe_components.append(component)

        return sum(fe_components)

    def _full_free_energy(
        self, beliefs: List[np.ndarray], observations: List[int]
    ) -> float:
        """Calculate full free energy without caching."""
        total_fe = 0.0

        for factor_idx in range(len(beliefs)):
            component = self._calculate_fe_component(
                factor_idx, beliefs, observations
            )
            total_fe += component

        return total_fe

    def _calculate_fe_component(
        self,
        factor_idx: int,
        beliefs: List[np.ndarray],
        observations: List[int],
    ) -> float:
        """Calculate free energy component for a specific factor."""
        # Simplified free energy calculation
        belief = beliefs[factor_idx]

        # Entropy term
        entropy = -np.sum(belief * log_stable(belief))

        # Expected log likelihood (simplified)
        expected_ll = 0.0
        for obs_idx, obs in enumerate(observations):
            if obs_idx < len(self.agent.A):
                A_matrix = self.agent.A[obs_idx]
                if A_matrix.ndim > 2:  # Handle multi-factor case
                    # Simplified marginalization
                    likelihood = np.mean(
                        A_matrix[obs], axis=tuple(range(1, A_matrix.ndim - 1))
                    )
                else:
                    likelihood = A_matrix[obs]
                expected_ll += np.sum(belief * log_stable(likelihood))

        return entropy - expected_ll

    def _detect_belief_changes(
        self,
        current_beliefs: List[np.ndarray],
        previous_beliefs: List[np.ndarray],
        threshold: float = 0.01,
    ) -> Set[int]:
        """Detect which belief factors changed significantly."""
        changed = set()

        for i, (curr, prev) in enumerate(
            zip(current_beliefs, previous_beliefs)
        ):
            # Calculate KL divergence as change measure
            kl_div = np.sum(curr * (log_stable(curr) - log_stable(prev)))
            if kl_div > threshold:
                changed.add(i)

        return changed

    def get_configuration(self) -> Dict[str, Any]:
        return {
            "state_size": self.state_size,
            "incremental_enabled": self.incremental_enabled,
            "num_factors": 2,
            "change_threshold": 0.01,
        }


class HierarchicalUpdateBenchmark(PyMDPBenchmark):
    """Benchmark hierarchical model updates with selective optimization."""

    def __init__(
        self, hierarchy_levels: int = 3, update_propagation: str = "selective"
    ):
        """Initialize hierarchical update benchmark.

        Args:
            hierarchy_levels: Number of levels in the hierarchy
            update_propagation: Update strategy ('selective' or 'full')
        """
        super().__init__(f"hierarchical_update_{update_propagation}")
        self.hierarchy_levels = hierarchy_levels
        self.update_propagation = update_propagation  # "selective" or "full"
        self.hierarchy = HierarchicalModel(hierarchy_levels)

    def setup(self):
        """Initialize hierarchical model structure."""
        if not PYMDP_AVAILABLE:
            return

        self.hierarchy.initialize()

    def run_iteration(self) -> Dict[str, Any]:
        """Run hierarchical update with selective propagation."""
        if not PYMDP_AVAILABLE:
            raise ImportError("PyMDP is required for this benchmark")

        # Simulate change at bottom level
        change_level = 0  # Bottom level
        change_magnitude = np.random.uniform(0.1, 0.5)

        if self.update_propagation == "selective":
            # Selective propagation
            start_time = time.perf_counter()
            levels_updated = self.hierarchy.selective_update(
                change_level, change_magnitude
            )
            selective_time = (time.perf_counter() - start_time) * 1000

            # For comparison, full propagation
            start_time = time.perf_counter()
            self.hierarchy.full_update(change_level, change_magnitude)
            full_time = (time.perf_counter() - start_time) * 1000

            computation_savings = (
                1 - levels_updated / self.hierarchy_levels
            ) * 100
            speedup = full_time / selective_time if selective_time > 0 else 1.0

        else:
            # Full propagation only
            start_time = time.perf_counter()
            levels_updated = self.hierarchy.full_update(
                change_level, change_magnitude
            )
            full_time = (time.perf_counter() - start_time) * 1000

            selective_time = full_time
            computation_savings = 0.0
            speedup = 1.0

        return {
            "full_update_time_ms": full_time,
            "selective_update_time_ms": selective_time,
            "speedup_factor": speedup,
            "computation_savings_percent": computation_savings,
            "levels_updated": levels_updated,
            "total_levels": self.hierarchy_levels,
            "change_magnitude": change_magnitude,
            "propagation_efficiency": levels_updated / self.hierarchy_levels,
        }

    def get_configuration(self) -> Dict[str, Any]:
        return {
            "hierarchy_levels": self.hierarchy_levels,
            "update_propagation": self.update_propagation,
        }


# Helper classes for tracking and caching


class ChangeTracker:
    """Tracks changes in observations and states."""

    def __init__(self):
        """Initialize change tracker with empty values and history."""
        self.previous_values = {}
        self.change_history = []

    def reset(self):
        """Reset tracker state."""
        self.previous_values.clear()
        self.change_history.clear()

    def track_change(self, key: str, value: Any) -> bool:
        """Track if a value changed."""
        changed = (
            key not in self.previous_values
            or self.previous_values[key] != value
        )
        self.previous_values[key] = value
        if changed:
            self.change_history.append((key, value))
        return changed


class StateHashTracker:
    """Tracks state hashes to detect changes."""

    def __init__(self):
        """Initialize state hash tracker with empty hash dictionary."""
        self.previous_hashes = {}

    def reset(self):
        """Reset hash tracker."""
        self.previous_hashes.clear()

    def compute_state_hash(self, beliefs: List[np.ndarray]) -> str:
        """Compute hash of current state."""
        hash_components = []
        for belief in beliefs:
            # Use first few moments as hash
            mean = np.mean(belief)
            var = np.var(belief)
            hash_components.extend([mean, var])
        return str(hash(tuple(hash_components)))

    def get_changed_factors(self, current_hash: str) -> Set[int]:
        """Get indices of factors that changed."""
        # Simplified - in practice would track per-factor hashes
        if (
            "global" not in self.previous_hashes
            or self.previous_hashes["global"] != current_hash
        ):
            self.previous_hashes["global"] = current_hash
            return {0, 1}  # Assume factors 0 and 1 changed
        return set()


class FreeEnergyCache:
    """Cache for free energy components."""

    def __init__(self):
        """Initialize free energy cache with empty dictionary."""
        self.cache = {}

    def reset(self):
        """Reset cache."""
        self.cache.clear()

    def get(self, factor_idx: int) -> Optional[float]:
        """Get cached free energy component."""
        return self.cache.get(factor_idx)

    def update(self, factor_idx: int, value: float):
        """Update cached free energy component."""
        self.cache[factor_idx] = value

    def size(self) -> int:
        """Get cache size."""
        return len(self.cache)


class HierarchicalModel:
    """Simplified hierarchical model for benchmarking."""

    def __init__(self, levels: int):
        """Initialize hierarchical model with specified number of levels.

        Args:
            levels: Number of hierarchical levels
        """
        self.levels = levels
        self.level_states = {}
        self.update_thresholds = [
            0.1,
            0.2,
            0.3,
        ]  # Thresholds for propagating updates

    def initialize(self):
        """Initialize hierarchical structure."""
        for level in range(self.levels):
            self.level_states[level] = np.random.rand(
                10
            )  # Random initial state

    def selective_update(
        self, change_level: int, change_magnitude: float
    ) -> int:
        """Perform selective hierarchical update."""
        levels_updated = 1  # Always update the change level

        # Propagate upward only if change is significant enough
        current_level = change_level
        current_magnitude = change_magnitude

        while current_level < self.levels - 1:
            threshold_idx = min(current_level, len(self.update_thresholds) - 1)
            if current_magnitude > self.update_thresholds[threshold_idx]:
                current_level += 1
                levels_updated += 1
                # Simulate update computation
                self._update_level(current_level, current_magnitude)
                # Magnitude decreases as it propagates up
                current_magnitude *= 0.7
            else:
                break

        return levels_updated

    def full_update(self, change_level: int, change_magnitude: float) -> int:
        """Perform full hierarchical update."""
        # Update all levels
        for level in range(self.levels):
            self._update_level(level, change_magnitude)
        return self.levels

    def _update_level(self, level: int, magnitude: float):
        """Update a specific level (simulate computation)."""
        # Simulate computational work
        for _ in range(10):
            dummy = np.random.rand(10, 10)
            dummy = softmax(dummy.flatten()).reshape(10, 10)


def run_selective_update_benchmarks():
    """Run comprehensive selective update benchmark suite."""
    suite = BenchmarkSuite()

    print(f"\n{'='*70}")
    print("SELECTIVE UPDATE OPTIMIZATION BENCHMARK SUITE")
    print(f"{'='*70}")
    print(
        "Testing optimization benefits of selective updates in PyMDP operations"
    )

    # Sparse observation benchmarks
    suite.add_benchmark(
        SparseObservationBenchmark(
            state_size=25, sparsity_level=0.1, selective_enabled=True
        )
    )
    suite.add_benchmark(
        SparseObservationBenchmark(
            state_size=25, sparsity_level=0.1, selective_enabled=False
        )
    )
    suite.add_benchmark(
        SparseObservationBenchmark(
            state_size=30, sparsity_level=0.3, selective_enabled=True
        )
    )
    suite.add_benchmark(
        SparseObservationBenchmark(
            state_size=30, sparsity_level=0.3, selective_enabled=False
        )
    )

    # Partial policy update benchmarks
    suite.add_benchmark(
        PartialPolicyUpdateBenchmark(
            state_size=20, num_policies=30, update_fraction=0.2
        )
    )
    suite.add_benchmark(
        PartialPolicyUpdateBenchmark(
            state_size=20, num_policies=30, update_fraction=0.5
        )
    )
    suite.add_benchmark(
        PartialPolicyUpdateBenchmark(
            state_size=25, num_policies=50, update_fraction=0.3
        )
    )

    # Incremental free energy benchmarks
    suite.add_benchmark(
        IncrementalFreeEnergyBenchmark(state_size=20, incremental_enabled=True)
    )
    suite.add_benchmark(
        IncrementalFreeEnergyBenchmark(
            state_size=20, incremental_enabled=False
        )
    )
    suite.add_benchmark(
        IncrementalFreeEnergyBenchmark(state_size=30, incremental_enabled=True)
    )
    suite.add_benchmark(
        IncrementalFreeEnergyBenchmark(
            state_size=30, incremental_enabled=False
        )
    )

    # Hierarchical update benchmarks
    suite.add_benchmark(
        HierarchicalUpdateBenchmark(
            hierarchy_levels=3, update_propagation="selective"
        )
    )
    suite.add_benchmark(
        HierarchicalUpdateBenchmark(
            hierarchy_levels=3, update_propagation="full"
        )
    )
    suite.add_benchmark(
        HierarchicalUpdateBenchmark(
            hierarchy_levels=5, update_propagation="selective"
        )
    )
    suite.add_benchmark(
        HierarchicalUpdateBenchmark(
            hierarchy_levels=5, update_propagation="full"
        )
    )

    # Run benchmarks
    results = suite.run_all(iterations=30)

    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    suite.save_results(f"selective_update_benchmark_results_{timestamp}.json")

    # Generate comprehensive analysis
    print(f"\n{'='*70}")
    print("SELECTIVE UPDATE OPTIMIZATION ANALYSIS")
    print(f"{'='*70}")

    # Analyze results by category
    sparse_results = [r for r in results if "sparse_observation" in r.name]
    policy_results = [r for r in results if "partial_policy" in r.name]
    fe_results = [r for r in results if "incremental_free_energy" in r.name]
    hierarchical_results = [
        r for r in results if "hierarchical_update" in r.name
    ]

    # Sparse observation analysis
    if sparse_results:
        print("\nüìä SPARSE OBSERVATION OPTIMIZATION:")
        selective_sparse = [r for r in sparse_results if "selective" in r.name]
        full_sparse = [r for r in sparse_results if "full" in r.name]

        if selective_sparse and full_sparse:
            avg_selective_time = np.mean(
                [r.mean_time_ms for r in selective_sparse]
            )
            avg_full_time = np.mean([r.mean_time_ms for r in full_sparse])
            overall_speedup = (
                avg_full_time / avg_selective_time
                if avg_selective_time > 0
                else 1.0
            )

            print(f"  Average selective time: {avg_selective_time:.2f} ms")
            print(f"  Average full time: {avg_full_time:.2f} ms")
            print(f"  Overall speedup: {overall_speedup:.2f}x")

            # Analyze computation savings
            savings = []
            for result in selective_sparse:
                if (
                    result.additional_metrics
                    and "computation_savings_percent"
                    in result.additional_metrics
                ):
                    savings.append(
                        result.additional_metrics[
                            "computation_savings_percent"
                        ]
                    )

            if savings:
                avg_savings = np.mean(savings)
                print(f"  Average computation savings: {avg_savings:.1f}%")

                if avg_savings >= 50:
                    print("  ‚úÖ Excellent optimization (>50% savings)")
                elif avg_savings >= 30:
                    print("  ‚úÖ Good optimization (>30% savings)")
                elif avg_savings >= 15:
                    print("  ‚ö†Ô∏è  Moderate optimization (>15% savings)")
                else:
                    print("  ‚ùå Limited optimization (<15% savings)")

    # Policy update analysis
    if policy_results:
        print("\nüîÑ PARTIAL POLICY UPDATE OPTIMIZATION:")
        speedups = []
        efficiencies = []

        for result in policy_results:
            if result.additional_metrics:
                speedup = result.additional_metrics.get("speedup_factor", 1.0)
                efficiency = result.additional_metrics.get(
                    "computation_savings_percent", 0.0
                )
                speedups.append(speedup)
                efficiencies.append(efficiency)

        if speedups:
            avg_speedup = np.mean(speedups)
            avg_efficiency = np.mean(efficiencies)

            print(f"  Average speedup: {avg_speedup:.2f}x")
            print(f"  Average efficiency gain: {avg_efficiency:.1f}%")

            if avg_speedup >= 3.0:
                print("  ‚úÖ Excellent policy optimization (>3x speedup)")
            elif avg_speedup >= 2.0:
                print("  ‚úÖ Good policy optimization (>2x speedup)")
            elif avg_speedup >= 1.5:
                print("  ‚ö†Ô∏è  Moderate policy optimization (>1.5x speedup)")
            else:
                print("  ‚ùå Limited policy optimization (<1.5x speedup)")

    # Free energy analysis
    if fe_results:
        print("\n‚ö° INCREMENTAL FREE ENERGY OPTIMIZATION:")
        incremental_results = [r for r in fe_results if "enabled" in r.name]
        full_fe_results = [r for r in fe_results if "disabled" in r.name]

        if incremental_results and full_fe_results:
            avg_incremental_time = np.mean(
                [r.mean_time_ms for r in incremental_results]
            )
            avg_full_fe_time = np.mean(
                [r.mean_time_ms for r in full_fe_results]
            )
            fe_speedup = (
                avg_full_fe_time / avg_incremental_time
                if avg_incremental_time > 0
                else 1.0
            )

            print(f"  Average incremental time: {avg_incremental_time:.2f} ms")
            print(
                f"  Average full calculation time: {avg_full_fe_time:.2f} ms"
            )
            print(f"  Free energy speedup: {fe_speedup:.2f}x")

            # Analyze accuracy maintenance
            accuracies = []
            for result in incremental_results:
                if (
                    result.additional_metrics
                    and "accuracy_maintained_percent"
                    in result.additional_metrics
                ):
                    accuracies.append(
                        result.additional_metrics[
                            "accuracy_maintained_percent"
                        ]
                    )

            if accuracies:
                avg_accuracy = np.mean(accuracies)
                print(f"  Average accuracy maintained: {avg_accuracy:.1f}%")

                if avg_accuracy >= 95:
                    print("  ‚úÖ Excellent accuracy preservation (>95%)")
                elif avg_accuracy >= 90:
                    print("  ‚úÖ Good accuracy preservation (>90%)")
                elif avg_accuracy >= 85:
                    print("  ‚ö†Ô∏è  Acceptable accuracy preservation (>85%)")
                else:
                    print("  ‚ùå Poor accuracy preservation (<85%)")

    # Hierarchical update analysis
    if hierarchical_results:
        print("\nüèóÔ∏è HIERARCHICAL UPDATE OPTIMIZATION:")
        selective_hier = [
            r for r in hierarchical_results if "selective" in r.name
        ]
        full_hier = [r for r in hierarchical_results if "full" in r.name]

        if selective_hier and full_hier:
            avg_selective_hier_time = np.mean(
                [r.mean_time_ms for r in selective_hier]
            )
            avg_full_hier_time = np.mean([r.mean_time_ms for r in full_hier])
            hier_speedup = (
                avg_full_hier_time / avg_selective_hier_time
                if avg_selective_hier_time > 0
                else 1.0
            )

            print(
                f"  Average selective time: {avg_selective_hier_time:.2f} ms"
            )
            print(
                f"  Average full propagation time: {avg_full_hier_time:.2f} ms"
            )
            print(f"  Hierarchical speedup: {hier_speedup:.2f}x")

            # Analyze propagation efficiency
            propagation_efficiencies = []
            for result in selective_hier:
                if (
                    result.additional_metrics
                    and "propagation_efficiency" in result.additional_metrics
                ):
                    propagation_efficiencies.append(
                        result.additional_metrics["propagation_efficiency"]
                    )

            if propagation_efficiencies:
                avg_prop_efficiency = np.mean(propagation_efficiencies)
                print(
                    f"  Average propagation efficiency: {avg_prop_efficiency:.2f}"
                )

                if avg_prop_efficiency <= 0.4:
                    print(
                        "  ‚úÖ Excellent selective propagation (‚â§40% levels updated)"
                    )
                elif avg_prop_efficiency <= 0.6:
                    print(
                        "  ‚úÖ Good selective propagation (‚â§60% levels updated)"
                    )
                elif avg_prop_efficiency <= 0.8:
                    print(
                        "  ‚ö†Ô∏è  Moderate selective propagation (‚â§80% levels updated)"
                    )
                else:
                    print(
                        "  ‚ùå Poor selective propagation (>80% levels updated)"
                    )

    # Overall optimization summary
    print("\nüéØ OVERALL SELECTIVE UPDATE OPTIMIZATION SUMMARY:")

    all_speedups = []
    all_savings = []

    for result in results:
        if result.additional_metrics:
            speedup = result.additional_metrics.get("speedup_factor")
            savings = result.additional_metrics.get(
                "computation_savings_percent"
            )

            if speedup and speedup > 1.0:
                all_speedups.append(speedup)
            if savings and savings > 0:
                all_savings.append(savings)

    if all_speedups:
        overall_avg_speedup = np.mean(all_speedups)
        max_speedup = np.max(all_speedups)
        print(f"  Overall average speedup: {overall_avg_speedup:.2f}x")
        print(f"  Maximum speedup achieved: {max_speedup:.2f}x")

    if all_savings:
        overall_avg_savings = np.mean(all_savings)
        max_savings = np.max(all_savings)
        print(
            f"  Overall average computation savings: {overall_avg_savings:.1f}%"
        )
        print(f"  Maximum savings achieved: {max_savings:.1f}%")

    # Performance classification
    if all_speedups and all_savings:
        if overall_avg_speedup >= 2.0 and overall_avg_savings >= 40:
            print("  üèÜ EXCELLENT selective update optimization achieved!")
        elif overall_avg_speedup >= 1.5 and overall_avg_savings >= 25:
            print("  ‚úÖ GOOD selective update optimization achieved!")
        elif overall_avg_speedup >= 1.2 and overall_avg_savings >= 15:
            print("  ‚ö†Ô∏è  MODERATE selective update optimization achieved!")
        else:
            print("  ‚ùå LIMITED selective update optimization achieved!")

    return results


if __name__ == "__main__":
    run_selective_update_benchmarks()
