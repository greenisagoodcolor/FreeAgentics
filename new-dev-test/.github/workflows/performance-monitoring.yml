name: "Performance Monitoring & Benchmarking"

on:
  schedule:
    # Run performance benchmarks daily at 3 AM UTC
    - cron: "0 3 * * *"

  workflow_dispatch:
    inputs:
      benchmark-type:
        description: "Type of benchmark to run"
        required: true
        default: "all"
        type: choice
        options:
          - all
          - api
          - database
          - memory
          - cpu
          - websocket

      comparison-branch:
        description: "Branch to compare against"
        required: false
        default: "main"

  pull_request:
    types: [opened, synchronize]
    paths:
      - "agents/**"
      - "api/**"
      - "coalitions/**"
      - "requirements*.txt"

env:
  PYTHON_VERSION: "3.12"
  BENCHMARK_RESULTS_DIR: "benchmark-results"
  PERFORMANCE_THRESHOLD: "10" # Performance regression threshold (%)

jobs:
  # ============================================================================
  # Performance Baseline
  # ============================================================================

  establish-baseline:
    name: "üìä Establish Baseline"
    runs-on: ubuntu-latest
    outputs:
      baseline-commit: ${{ steps.baseline.outputs.commit }}
      baseline-exists: ${{ steps.baseline.outputs.exists }}
    steps:
      - name: "üì• Checkout Code"
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: "üîç Find Baseline"
        id: baseline
        run: |
          # Determine baseline branch/commit
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            BASELINE_BRANCH="${{ github.base_ref }}"
          else
            BASELINE_BRANCH="${{ github.event.inputs.comparison-branch || 'main' }}"
          fi

          BASELINE_COMMIT=$(git rev-parse origin/$BASELINE_BRANCH)
          echo "commit=$BASELINE_COMMIT" >> $GITHUB_OUTPUT

          # Check if we have baseline data
          if [ -f "${{ env.BENCHMARK_RESULTS_DIR }}/baseline-$BASELINE_COMMIT.json" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "### Using existing baseline from commit: $BASELINE_COMMIT" >> $GITHUB_STEP_SUMMARY
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "### No baseline found, will create new baseline" >> $GITHUB_STEP_SUMMARY
          fi

  # ============================================================================
  # API Performance Tests
  # ============================================================================

  api-performance:
    name: "‚ö° API Performance"
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.benchmark-type == 'all' || github.event.inputs.benchmark-type == 'api' || github.event_name != 'workflow_dispatch' }}
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: freeagentics_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
        ports:
          - 6379:6379

    steps:
      - name: "üì• Checkout Code"
        uses: actions/checkout@v4

      - name: "üîß Setup Python"
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: "üì¶ Install Dependencies"
        run: |
          python -m venv venv
          source venv/bin/activate
          pip install -r requirements-ci.txt
          pip install locust pytest-benchmark httpx aiohttp

      - name: "üöÄ Start API Server"
        env:
          DATABASE_URL: "postgresql://test:test@localhost:5432/freeagentics_test"
          REDIS_URL: "redis://localhost:6379"
        run: |
          source venv/bin/activate
          PYTHONPATH="." uvicorn api.main:app --host 0.0.0.0 --port 8000 &

          # Wait for server to start
          for i in {1..30}; do
            if curl -f http://localhost:8000/health; then
              break
            fi
            sleep 1
          done

      - name: "‚ö° Run API Benchmarks"
        run: |
          source venv/bin/activate
          mkdir -p ${{ env.BENCHMARK_RESULTS_DIR }}

          # Run various API performance tests
          python scripts/benchmark_api.py \
            --endpoints all \
            --duration 60 \
            --users 100 \
            --spawn-rate 10 \
            --output ${{ env.BENCHMARK_RESULTS_DIR }}/api-benchmark.json

      - name: "üìä Generate API Performance Report"
        run: |
          echo "### API Performance Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          python scripts/analyze_benchmarks.py \
            --input ${{ env.BENCHMARK_RESULTS_DIR }}/api-benchmark.json \
            --format markdown >> $GITHUB_STEP_SUMMARY

      - name: "üì§ Upload API Results"
        uses: actions/upload-artifact@v4
        with:
          name: api-performance-${{ github.run_id }}
          path: ${{ env.BENCHMARK_RESULTS_DIR }}/api-benchmark.json

  # ============================================================================
  # Database Performance Tests
  # ============================================================================

  database-performance:
    name: "üóÑÔ∏è Database Performance"
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.benchmark-type == 'all' || github.event.inputs.benchmark-type == 'database' }}
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: freeagentics_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: "üì• Checkout Code"
        uses: actions/checkout@v4

      - name: "üîß Setup Python"
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: "üì¶ Install Dependencies"
        run: |
          python -m venv venv
          source venv/bin/activate
          pip install -r requirements-ci.txt
          pip install pgbench psycopg2-binary

      - name: "üóÑÔ∏è Setup Test Database"
        env:
          DATABASE_URL: "postgresql://test:test@localhost:5432/freeagentics_test"
        run: |
          source venv/bin/activate
          # Run migrations
          alembic upgrade head

          # Seed with test data
          python scripts/seed_performance_data.py --records 100000

      - name: "‚ö° Run Database Benchmarks"
        env:
          DATABASE_URL: "postgresql://test:test@localhost:5432/freeagentics_test"
        run: |
          source venv/bin/activate
          mkdir -p ${{ env.BENCHMARK_RESULTS_DIR }}

          # Run database performance tests
          python scripts/benchmark_database.py \
            --queries all \
            --iterations 1000 \
            --concurrent-connections 50 \
            --output ${{ env.BENCHMARK_RESULTS_DIR }}/db-benchmark.json

      - name: "üìä Query Performance Analysis"
        env:
          DATABASE_URL: "postgresql://test:test@localhost:5432/freeagentics_test"
        run: |
          echo "### Database Performance Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Analyze slow queries
          psql $DATABASE_URL -c "
            SELECT query, mean_exec_time, calls
            FROM pg_stat_statements
            WHERE mean_exec_time > 10
            ORDER BY mean_exec_time DESC
            LIMIT 10
          " >> $GITHUB_STEP_SUMMARY || true

      - name: "üì§ Upload Database Results"
        uses: actions/upload-artifact@v4
        with:
          name: database-performance-${{ github.run_id }}
          path: ${{ env.BENCHMARK_RESULTS_DIR }}/db-benchmark.json

  # ============================================================================
  # Memory Performance Tests
  # ============================================================================

  memory-performance:
    name: "üß† Memory Performance"
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.benchmark-type == 'all' || github.event.inputs.benchmark-type == 'memory' }}
    steps:
      - name: "üì• Checkout Code"
        uses: actions/checkout@v4

      - name: "üîß Setup Python"
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: "üì¶ Install Dependencies"
        run: |
          python -m venv venv
          source venv/bin/activate
          pip install -r requirements-ci.txt
          pip install memory-profiler pympler tracemalloc-ng psutil

      - name: "üß† Run Memory Benchmarks"
        run: |
          source venv/bin/activate
          mkdir -p ${{ env.BENCHMARK_RESULTS_DIR }}

          # Run memory profiling tests
          python scripts/benchmark_memory.py \
            --scenarios all \
            --agents 1000 \
            --duration 300 \
            --output ${{ env.BENCHMARK_RESULTS_DIR }}/memory-benchmark.json

      - name: "üìä Memory Usage Analysis"
        run: |
          echo "### Memory Performance Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          python scripts/analyze_memory.py \
            --input ${{ env.BENCHMARK_RESULTS_DIR }}/memory-benchmark.json \
            --format markdown >> $GITHUB_STEP_SUMMARY

      - name: "üîç Memory Leak Detection"
        run: |
          source venv/bin/activate

          # Run memory leak detection
          python scripts/detect_memory_leaks.py \
            --modules agents,api,coalitions \
            --iterations 100 \
            --threshold 1MB

      - name: "üì§ Upload Memory Results"
        uses: actions/upload-artifact@v4
        with:
          name: memory-performance-${{ github.run_id }}
          path: ${{ env.BENCHMARK_RESULTS_DIR }}/memory-benchmark.json

  # ============================================================================
  # CPU Performance Tests
  # ============================================================================

  cpu-performance:
    name: "üíª CPU Performance"
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.benchmark-type == 'all' || github.event.inputs.benchmark-type == 'cpu' }}
    steps:
      - name: "üì• Checkout Code"
        uses: actions/checkout@v4

      - name: "üîß Setup Python"
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: "üì¶ Install Dependencies"
        run: |
          python -m venv venv
          source venv/bin/activate
          pip install -r requirements-ci.txt
          pip install py-spy scalene line_profiler

      - name: "üíª Run CPU Benchmarks"
        run: |
          source venv/bin/activate
          mkdir -p ${{ env.BENCHMARK_RESULTS_DIR }}

          # Run CPU intensive benchmarks
          python scripts/benchmark_cpu.py \
            --algorithms all \
            --iterations 1000 \
            --complexity high \
            --output ${{ env.BENCHMARK_RESULTS_DIR }}/cpu-benchmark.json

      - name: "üî• CPU Profiling"
        run: |
          source venv/bin/activate

          # Profile hot paths
          py-spy record -o ${{ env.BENCHMARK_RESULTS_DIR }}/cpu-profile.svg \
            --duration 30 \
            -- python scripts/simulate_workload.py

      - name: "üìä CPU Performance Analysis"
        run: |
          echo "### CPU Performance Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          python scripts/analyze_cpu.py \
            --input ${{ env.BENCHMARK_RESULTS_DIR }}/cpu-benchmark.json \
            --profile ${{ env.BENCHMARK_RESULTS_DIR }}/cpu-profile.svg \
            --format markdown >> $GITHUB_STEP_SUMMARY

      - name: "üì§ Upload CPU Results"
        uses: actions/upload-artifact@v4
        with:
          name: cpu-performance-${{ github.run_id }}
          path: |
            ${{ env.BENCHMARK_RESULTS_DIR }}/cpu-benchmark.json
            ${{ env.BENCHMARK_RESULTS_DIR }}/cpu-profile.svg

  # ============================================================================
  # WebSocket Performance Tests
  # ============================================================================

  websocket-performance:
    name: "üîå WebSocket Performance"
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.benchmark-type == 'all' || github.event.inputs.benchmark-type == 'websocket' }}
    services:
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
        ports:
          - 6379:6379

    steps:
      - name: "üì• Checkout Code"
        uses: actions/checkout@v4

      - name: "üîß Setup Python"
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: "üì¶ Install Dependencies"
        run: |
          python -m venv venv
          source venv/bin/activate
          pip install -r requirements-ci.txt
          pip install websocket-client locust websockets

      - name: "üöÄ Start WebSocket Server"
        env:
          REDIS_URL: "redis://localhost:6379"
        run: |
          source venv/bin/activate
          PYTHONPATH="." uvicorn api.main:app --host 0.0.0.0 --port 8000 &
          sleep 5

      - name: "üîå Run WebSocket Benchmarks"
        run: |
          source venv/bin/activate
          mkdir -p ${{ env.BENCHMARK_RESULTS_DIR }}

          # Run WebSocket performance tests
          python scripts/benchmark_websocket.py \
            --connections 1000 \
            --messages-per-second 100 \
            --duration 60 \
            --output ${{ env.BENCHMARK_RESULTS_DIR }}/websocket-benchmark.json

      - name: "üìä WebSocket Performance Analysis"
        run: |
          echo "### WebSocket Performance Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          python scripts/analyze_websocket.py \
            --input ${{ env.BENCHMARK_RESULTS_DIR }}/websocket-benchmark.json \
            --format markdown >> $GITHUB_STEP_SUMMARY

      - name: "üì§ Upload WebSocket Results"
        uses: actions/upload-artifact@v4
        with:
          name: websocket-performance-${{ github.run_id }}
          path: ${{ env.BENCHMARK_RESULTS_DIR }}/websocket-benchmark.json

  # ============================================================================
  # Performance Comparison & Regression Detection
  # ============================================================================

  performance-analysis:
    name: "üìä Performance Analysis"
    needs:
      [
        establish-baseline,
        api-performance,
        database-performance,
        memory-performance,
        cpu-performance,
        websocket-performance,
      ]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: "üì• Checkout Code"
        uses: actions/checkout@v4

      - name: "üîß Setup Python"
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: "üì• Download All Results"
        uses: actions/download-artifact@v4
        with:
          path: performance-artifacts

      - name: "üìä Aggregate Results"
        run: |
          mkdir -p ${{ env.BENCHMARK_RESULTS_DIR }}

          # Combine all benchmark results
          python scripts/aggregate_benchmarks.py \
            --input-dir performance-artifacts \
            --output ${{ env.BENCHMARK_RESULTS_DIR }}/combined-results.json

      - name: "üîç Regression Detection"
        id: regression
        run: |
          if [ "${{ needs.establish-baseline.outputs.baseline-exists }}" = "true" ]; then
            # Compare with baseline
            python scripts/detect_regression.py \
              --current ${{ env.BENCHMARK_RESULTS_DIR }}/combined-results.json \
              --baseline ${{ env.BENCHMARK_RESULTS_DIR }}/baseline-${{ needs.establish-baseline.outputs.baseline-commit }}.json \
              --threshold ${{ env.PERFORMANCE_THRESHOLD }} \
              --output regression-report.json

            # Check if regression detected
            if [ -f "regression-detected" ]; then
              echo "regression=true" >> $GITHUB_OUTPUT
            else
              echo "regression=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "regression=false" >> $GITHUB_OUTPUT
          fi

      - name: "üìà Generate Performance Report"
        run: |
          echo "# üìä Performance Benchmark Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Date**: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "**Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "**Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Summary table
          echo "## Performance Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Category | Status | Key Metrics |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|--------|-------------|" >> $GITHUB_STEP_SUMMARY

          # Add results from each category
          for category in api database memory cpu websocket; do
            if [ -f "performance-artifacts/${category}-performance-${{ github.run_id }}/${category}-benchmark.json" ]; then
              python scripts/summarize_category.py \
                --category $category \
                --file "performance-artifacts/${category}-performance-${{ github.run_id }}/${category}-benchmark.json" \
                --format table-row >> $GITHUB_STEP_SUMMARY
            fi
          done

          echo "" >> $GITHUB_STEP_SUMMARY

          # Regression analysis
          if [ "${{ steps.regression.outputs.regression }}" = "true" ]; then
            echo "## ‚ö†Ô∏è Performance Regression Detected" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            python scripts/format_regression_report.py \
              --input regression-report.json \
              --format markdown >> $GITHUB_STEP_SUMMARY
          else
            echo "## ‚úÖ No Performance Regression" >> $GITHUB_STEP_SUMMARY
            echo "All metrics within acceptable thresholds." >> $GITHUB_STEP_SUMMARY
          fi

      - name: "üíæ Store Benchmark Results"
        if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop'
        run: |
          # Store as new baseline if on main branch
          cp ${{ env.BENCHMARK_RESULTS_DIR }}/combined-results.json \
             ${{ env.BENCHMARK_RESULTS_DIR }}/baseline-${{ github.sha }}.json

          # Keep last 30 baselines
          ls -t ${{ env.BENCHMARK_RESULTS_DIR }}/baseline-*.json | tail -n +31 | xargs rm -f || true

      - name: "üì§ Upload Final Report"
        uses: actions/upload-artifact@v4
        with:
          name: performance-report-${{ github.run_id }}
          path: |
            ${{ env.BENCHMARK_RESULTS_DIR }}/combined-results.json
            regression-report.json

      - name: "üí¨ Comment on PR"
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // Read the performance summary
            const summary = fs.readFileSync('performance-summary.md', 'utf8');

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Performance Benchmark Report')
            );

            const body = `## üìä Performance Benchmark Report\n\n${summary}`;

            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: "üö® Fail on Regression"
        if: steps.regression.outputs.regression == 'true' && github.event_name == 'pull_request'
        run: |
          echo "‚ùå Performance regression detected. Please review the performance report."
          exit 1
